{
    "0": [
        {
            "index": 0,
            "text": "Abstract—Advances in the ﬁeld of Machine Learning andDeep Neural Networks (DNNs) has enabled rapid developmentof sophisticated and autonomous systems. However, the inherentcomplexity to rigorously assure the safe operation of such systemshinders their real-world adoption in safety-critical domains suchas aerospace and medical devices. Hence, there is a surge ininterest to explore the use of advanced mathematical techniquessuch as formal methods to address this challenge. In fact, theinitial results of such efforts are promising. Along these lines,we propose the use of quantiﬁer elimination (QE) — a formalmethod technique, as a complimentary technique to the state-of-the-art static analysis and veriﬁcation procedures. Using anairborne collision avoidance DNN as a case example, we illustratethe use of QE to formulate the precise range forward propagationthrough a network as well as analyze its robustness. We discussthe initial results of this ongoing work and explore the futurepossibilities of extending this approach and/or integrating it withother approaches to perform advanced safety assurance of DNNs.",
            "refs": []
        },
        {
            "index": 1,
            "text": "Recently, there is a tremendous surge of interest withinthe aerospace community to leverage advances in MachineLearning (ML) to develop sophisticated software for large,autonomous avionic systems such as unmanned aircrafts. Infact, the inherent ability of the modern structurally complexcomputing systems such as Deep Neural Networks (DNN),that automatically learn and generalize behaviors based on aset of training data rather than explicit programming basedon requirements, makes it a natural choice for developingautonomous components for aircraft. However, there is a wide-spread apprehension about deploying such systems in the real-world since it has not been possible to rigorously interpretand assure the safe functional boundaries and behaviors ofthe DNNs due to their structural complexity and behaviouralimmensity . For instance, analyzing the robustnessof DNNs against adversarial attacks  — smallperturbations to inputs that lead to unsafe outputs, remainsas an open safety assurance concern.",
            "refs": [
                3,
                4,
                5,
                6
            ]
        },
        {
            "index": 2,
            "text": "The use of mathematical techniques such as formal methodshas been recently demonstrated as a promising directiontowards addressing this challenge. Several formal approacheshave been proposed so far to measure DNN’s robustnessand resilience against adversarial inputs. Namely, recent ap-proaches such as Reluplex  have used linear pro-gramming (LP) and satsiﬁability modulo theory (SMT) solversto verify adversarial robustness of DNNs that use piece-wise linear activation function such as Rectiﬁed Linear Units",
            "refs": [
                7,
                8,
                9
            ]
        },
        {
            "index": 3,
            "text": "(ReLU). Further, DeepSafe  studies the safe boundariesof adversarial robustness guided by data, relying on clusteringto identify well-deﬁned geometric regions as candidate saferegions. Then it leverages Reluplex veriﬁcation results forconﬁrmation. Another distinctive approach in the area, Relu-Val , combines the search for concrete counterexampleswith layer-by-layer reachability analysis of the DNN. Thisapproach uses interval arithmetic to symbolically compute therange bounds on the intermediate nodes and outputs of theDNN. This has allowed ReluVal to verify a class of DNNproperties approximately 200× faster than Reluplex .However, the underlying algorithm of ReluVal computes theover-approximation of the ranges of nodes in a DNN that,unfortunately, has not been shown to be able to assess theadversarial robustness of the DNN.",
            "refs": [
                11
            ]
        },
        {
            "index": 4,
            "text": "In this paper, we propose a quantiﬁer elimination (QE)based static analysis of DNNs with ReLU activation, as a com-plimentary technique to the state-of-the-art DNN veriﬁcationprocedures. QE is a powerful formal technique for gaininginsight into problems involving complex logic expressions. Inthe recent past, QE has effectively been used to derive thestrongest system property from components properties in thecompositional veriﬁcation of traditional systems .",
            "refs": [
                13
            ]
        },
        {
            "index": 5,
            "text": "Envisioning a DNN as a system composed of layers of con-nected functional nodes, our idea is to formulate the problemof precise range computation of DNN as a QE problem andleverage QE solvers such as Redlog  to derive the range. Inaddition to property veriﬁcation, the QE-based analysis allowsderivation of precise regions in the input space of a DNN fora desired output robustness measure. We adopt the forwardrange propagation approach, that has also been used for layer-by-layer reachability analysis in ReluVal,linear-behavioral neurons in each layer for simpliﬁcation of onwardcomputation. The advantage of using QE (over approachessuch as ReluVal) is that QE can compute the precise rangeat the existence of piece-wise linear behavioral neurons, i.e.,non-convex scenarios.",
            "refs": []
        },
        {
            "index": 6,
            "text": "As a proof-of-concept, we developed a prototype implemen-tation and evaluated our approach using pre-trained DNNs ofnext-generation Airborne Collision Avoidance System for un-manned aircraft (ACAS Xu networks)  as a case example.To cope with scalability issues we encountered when runningthe experiments, we used several heuristics and ﬁne-tuningtechniques, that we discuss later in the paper. Since ACASXu networks have been benchmarks for evaluation of existing",
            "refs": [
                15
            ]
        },
        {
            "index": 7,
            "text": "The rest of the paper is organized as follows. Section II givesthe brief background of DNN and related work on current ver-iﬁcation progress, and the concept of QE. Section III describesourforward range propagation and veriﬁcation approachbuilt upon QE formulation of range computation. Section IVpresents our prototype implementation and initial test results.Section V concludes this paper and envisions the potentialperformance improvement and future work of integrating QEwith other formal network veriﬁcation approaches.",
            "refs": []
        },
        {
            "index": 8,
            "text": "A typical feed-forward deep neural network is a layeredstructure of connected nodes called neurons, where eachneuron reﬁnes and extracts information from values com-puted by neurons in the previous layer. An N-layer neuralnetwork, as shown in Figure 1, maps the input vector (cid:126)xto output(s) (cid:126)y, through a composed function (cid:126)y = f ((cid:126)x) :=f N (f N−1(. . . f 2(f 1((cid:126)x)) . . . )), where f k∈[1,N ] denotes thefeed-forward computation by the kth layer. Typically, f kconsists of a linear transformation (deﬁned by a weight matrixand a bias vector) of the output values of f k−1 (f 0((cid:126)x) = (cid:126)x forthe network input layer), and a non-linear activation functionto the weighted sum. In this paper, we will consider DNNswith piece-wise linear ReLU-activation function (ReLU (x) =max(0, x) element-wise).",
            "refs": []
        },
        {
            "index": 9,
            "text": "hence is met in many domains. However, in safety-criticalsystems it is desirable, and sometimes mandatory, that it isrigorously assured that certain properties hold irrespective ofany input. While the structural complexity of DNNs preventsmathematical interpretation of the DNN’s behaviours, it iswell-known that testing is insufﬁcient to conclude the non-existence of inputs leading to erroneous results. Hence, there isa strong need to explore alternate means of analysis proceduressuch as formal methods that have being extensively used intraditional systems.",
            "refs": []
        },
        {
            "index": 10,
            "text": "where Encodingnet is the encoding of inherent constraintsof a particular DNN. Typically, Encodingnet can be theconjunction of constraints from every neuron, including thelinear constraint from the weighted sum function and thepiece-wise linear constraint from the ReLU activation function,as exempliﬁed in .",
            "refs": [
                16
            ]
        },
        {
            "index": 11,
            "text": "One class of the desired input-output properties of DNNs isrobustness against adversarial attacks, i.e., that small input per-turbations cause major deviations (such as mis-classiﬁcation)in the networks output. A DNN’s mis-classiﬁcation lacksprecise mathematical deﬁnition, since the ground truth canbe subjective. For example, image mis-classiﬁcation (outsidethe training data set) is often considered as inconsistencywith human eye classiﬁcation. Human eye, although beingclose to, rarely qualiﬁes to provide the ground truth. However,adversarial attacks often practically explore and exploit the un-smoothness of a trained network, to carefully craft adversarialinputs. It is due to the fact that DNNs are trained over a ﬁniteset of data and are therefore unable to generalize over theentire input space. Dually, a DNN may be retrained to increaseits robustness by improving its smoothness . Before wedive into veriﬁcation, we introduce two common deﬁnitionsof adversarial robustness below. For a DNN (a classiﬁer) withn neurons at the output layer, each output neuron correspondsto an output label l ∈ {l1, . . . , ln}. The output label of aninput sample (cid:126)x, denoted by L((cid:126)x), is decided based on thecomparison among the output neurons’ values. For example,one can specify L((cid:126)x) = li iff li > lj,∀j∈[1,n],j(cid:54)=i.Deﬁnition 1. A DNN is δ-locally-robust at point (cid:126)x0 iff||(cid:126)x − (cid:126)x0|| ≤ δ ⇒ L((cid:126)x) = L((cid:126)x0).",
            "refs": [
                17
            ]
        },
        {
            "index": 12,
            "text": "Def. 1  requires that the DNN assigns the samelabel to two input points close enough to each other. Notethat, this deﬁnition is meaningful only when measured locallyregarding the proper choice of the reference point (cid:126)x0, since",
            "refs": [
                7,
                16,
                18
            ]
        },
        {
            "index": 13,
            "text": "the only type of DNN that satisﬁes this property globally willnaively output the same label for all its inputs (the distancebetween any two ﬁnite input points can be partitioned intoﬁnite number of δ segments). On the other hand, since theDNN classiﬁcation lacks global ground truth, it is impracticalto force a DNN to outputthe same label within the δ-neighbourhood of (cid:126)x0. Therefore, we also consider anotherdeﬁnition of DNN robustness that is more inherent to theexpected behavior of a DNN itself as follows.Deﬁnition 2. A DNN is (δ, \u0001)-locally-robust at point (cid:126)x0 iff∀(cid:126)x, ||(cid:126)x − (cid:126)x0|| ≤ δ ⇒ ∀f∈{f1,...,fn},||f ((cid:126)x) − f ((cid:126)x0)|| ≤ \u0001,where fi∈{1,...,n}((cid:126)x) denotes the output value of the i-th outputneuron given input (cid:126)x.",
            "refs": []
        },
        {
            "index": 14,
            "text": "Def. 2  addresses the smoothness of the input-outputfunctions locally around a reference input (cid:126)x0. Meanwhile, itis also expected to hold for arbitrary reference input (cid:126)x0 forsome \u0001 (although it is still practically hard to be veriﬁed atthe global scale even with the state-of-the-art tools).",
            "refs": [
                8
            ]
        },
        {
            "index": 15,
            "text": "There are usually two directions to approach a veriﬁcationproblem: to falsify a property or prove it. It is sufﬁcient toﬁnd one counterexample to falsify a property. Some methodsimplement searching algorithms for counterexample(s). Forexample, in convex optimization simplex is an efﬁcient algo-rithm that moves from one vertex to another along the edgesin search for the optimal solution. Reaching one vertex thatviolates the property leads to algorithm termination along withthe vertex identiﬁed as a concrete counterexample. In DNNs,the ReLU activation function renders the constraint set non-convex. To address such non-convex optimization problem,Reluplex uses a modiﬁed simplex algorithm aided by LPand SMT solvers. During the search, Reluplex either ﬁndsa concrete counterexample or concludes that there is none.",
            "refs": []
        },
        {
            "index": 16,
            "text": "problems could be a major factor towards scalability enhance-ment. Therefore, we believe that, just like the training of aDNN usually involves a lot of experimenting and parametertuning, the veriﬁcation problems should also be addressed ina broad framework integrating a rich set of approaches andformal methods tools. For a more recent and comprehensivesurvey of existing algorithms for verifying DNNs, readers canrefer to . To add to that effort, we propose a quantiﬁerelimination (QE) based range propagation method,in thefollowing section, as a complimentary formal method to state-of-the-art veriﬁcation approaches.",
            "refs": []
        },
        {
            "index": 17,
            "text": "Quantiﬁer elimination (QE) is a powerful technique forgaining insight, through simpliﬁcation, into problems involv-ing logic expressions in various theories. A theory admitsquantiﬁer elimination if and only if for every quantiﬁedformula α in that theory, there exists another quantiﬁer-freeformula αQF that is logically equivalent to it. For example,in the domain of real numbers, ∃x(y > x2) is logicallyequivalent to y > 0 (since x2 is always positive). QE isthe procedure of deriving an equivalent quantiﬁer-free formulafrom a quantiﬁed formula, as the former can be seen as theresidue of the later after the elimination of the quantiﬁers aswell as the quantiﬁed variables. In other words, QE can beviewed as a dimensionality reduction method of quantiﬁeddimensions. Since it has been proven by Tarski  thatthe real closed ﬁeld admits QE, it is suitable for solving theproblems of real-world applications.",
            "refs": []
        },
        {
            "index": 18,
            "text": "Following the ﬁrst implementable QE procedure in 1975 byCollins , called the cylindrical algebraic decomposition(CAD), the QE-based techniques and tools have undergonetremendous enrichment over the past few decades and the ef-forts made along the way have contributed to newer additions.In particular, specialized procedures for restricted problemclasses led to newer, more advanced QE procedures, docu-mented in tools such as Mathematica  and Redlog .For example, Redlog implements virtual substitution  andpartial CAD  algorithms, that work for formulae wherethe degrees of the quantiﬁed variables are small.",
            "refs": [
                21,
                22
            ]
        },
        {
            "index": 19,
            "text": "As part of our exploration of aforementioned distinguishedcapabilities of QE, we established in , that QE servesas a composition calculus, and applies to compositional ver-iﬁcation, a technique being developed to cope with state-space explosion in concurrent systems . Essentiallythe strategy of divide-and-conquer is employed where oneﬁrst establishes the properties of the system components, andthen uses those to establish the overarching properties ofa complex system. Supposing a system is composed of Ncomponents, the property contract of the ith component canbe expressed as Ai ⇒ Gi, where Ai (the “assumption”) andGi (the “guarantee”) are both expressed as 1st-order logicformulae over the set of component variables. Then the setof all the system behaviors is constrained by the conjunctioni=1(Ai ⇒ Gi). Underthese components contracts, the “strongest system property”that can be claimed by the system, can be obtained byexistentially quantifying the system’s internal variables in theconjunct of component contracts with the constraints resultingfrom the connectivity relation among the components. Thuswe established that QE serves as a foundation for propertycontract composition. Then to check whether a system satisﬁesa postulated property, we only need to check if the postulatedproperty is implied by the aforementioned strongest systemproperty. This itself can be cast as another QE problem.",
            "refs": [
                25
            ]
        },
        {
            "index": 20,
            "text": "We extend our idea of contract composition for deriving thestrongest system property to formulate the range computationof a ReLU-DNN neuron into a QE problem. Consider a ReLU-DNN of N layers with input (cid:126)x, output (cid:126)y, and label L. LetZ be the set of intermediate weighted sum variables and Abe the set of the intermediate activation variables. Withoutloss of generality, we present out approach over the rangecomputation of a single output variable y ∈ (cid:126)y. Because all theoutput variables are structurally parallel and share the sameupstream N − 1 layers, the same range computation techniquecan be applied to each of them. Let the encoding of DNNregarding to the particular output y (of all its related weightedsum and activation constraints), Encodingnet, be expressedas the conjunction of all the predicates over (cid:126)x, y, Z, and A.Let C(cid:126)x be the set of speciﬁed input constraints (ranges).",
            "refs": []
        },
        {
            "index": 21,
            "text": "(1)where ∃ over a vector (resp. set) denotes existentially quantify-ing every element variable of the vector (resp. set). Note that,(1) is the DNN interpretation of the strongest system propertyformulation in . Logically, since all the variables but y in(1) are quantiﬁed, the quantiﬁer-free equivalence of (1) is aconjunction and/or disjunction of linear predicates over y only.",
            "refs": []
        },
        {
            "index": 22,
            "text": "Proposition 1. (1) is the precise range property of y underinput range constraints C(cid:126)x.Proof. Let Rprecise(C(cid:126)x, y) denote the actual precise rangeproperty of y given C(cid:126)x. Following Proposition 1 in , (1)is a system property of the DNN, i.e., all y values of the DNNunder C(cid:126)x must satisfy (1), then we have Rprecise(C(cid:126)x, y) ⇒(1). Meanwhile, (1) is the strongest system property, meaningthat it implies any other system property. We then also have(1) ⇒ Rprecise(C(cid:126)x, y). The mutual implication between (1)and Rprecise(C(cid:126)x, y) proves that they are in fact equivalent.",
            "refs": []
        },
        {
            "index": 23,
            "text": "The formulation of (1) and Proposition 1 are the corner-stones of our QE-based DNN range propagation approach.However, Encodingnet contains a large size of variables andconstraints, especially the piece-wise linear constraints thatrender the problem non-convex. The state-of-the art QE solversare insufﬁcient to process it naively. In the rest of the section,we brieﬂy present a layer-by-layer forward range propagation(a sequence of range computations, each of which has its ownformulation as in (1)) similar to ReluVal , along with a setof heuristics on-the-ﬂy to reduce the size of each individualQE formulation, and eventually conclude the precise range ofy for a benchmark-scale network.Basics of Forward Range Propagation. In our setting, eachhidden neuron is splitinto a weighted-sum node and anactivation node. We denote the output range (resp. value) ofthe weight sum node as the z-range (resp. z-value), and theoutput range (resp. value) of the activation node as the a-range (resp. a-value). The forward range propagation startsfrom the ﬁrst hidden layer, where the z-range of each neuroncan be formulated similarly to (1) with the smallest sub-graphencoding including only the input constraints and the weightedsum node constraint. Since all neurons in the same layershare the same upstream sub-graph, their z-ranges naturallyﬁt multi-threaded parallel computation. Theoretically, the timecost for range propagation of an entire layer with n neuronscan be reduced to that of a single neuron by running nthreads in parallel. The a-range of a neuron can be triviallycomputed from its z-range following the ReLU function. Suchcomputation steps are forward progressed layer-by-layer untilthe ranges of the target output neurons are computed.On-the-ﬂy Pruning for Scalability. The scalability of theprecise range propagation largely depends the QE solver’sperformance on the encoding of the network. The classesof local adversarial robustness properties veriﬁcation haverelatively small input ranges (δ-neighborhood of a samplepoint). In this premise, a majority of the hidden neuronseither behave purely linearly or even have a constant zerooutput. The above forward propagation approach identiﬁessuch neurons and simpliﬁes the network behavioral structureas much as possible. In our preliminary tests, we found thatthe major factor causing our QE solver Redlog timeout, isthe number of disjunctive predicate clauses in the networkencoding. This corresponds to the number of branching neu-rons remaining in the simpliﬁed network behavioral structure.We observed that Redlog can handle a single neuron precise",
            "refs": [
                11
            ]
        },
        {
            "index": 24,
            "text": "range computation, with 6∼8 branching neurons distributedacross different upstream layers, in less than a few tens ofseconds. However, one more such neuron may suddenly leadsto more than 1,000 seconds execution time or even timeout.The on-the-ﬂy pruning (identifying and then simplifying) ofthe linear part of the network behavioral structure results ina much computationally affordable residual of the network.Note that, the precise range computation of each neuron inlayer l requires the encoding of all its upstream sub-graphof the behavioral structure rather than justthe ranges ofneurons in layer l − 1. Failure to do so will lose tracking thecorrelation among the neurons, thus introducing large over-approximation errors similar to the naive interval propagationnoted by ReluVal in .Over-approximation. Many veriﬁcation problems allow over-approximation as long as the property still holds with areasonable over-approximation precision. In network noderange computation, one can replace the set of constraints ofa branching neuron, i.e., constraints of both upstream andits functionality, with only its propagated a-range to over-approximate the downstream range computation. This effect isknown as the dependency problem  and is studied in in the network context. A new effect for our QE-based rangepropagation is that the over-approximation may cause morebranching neurons downstream than there actually exists. Wepropose a few techniques to ﬁne-tune the trade-off betweencomputation efﬁciency and precision. The over-approximationshall be used only for the branching neurons, since branchingbehavior is the major hinder of QE solving. And/Or onecan over-approximate a subset of the branching neurons,leaving a small number of branching neurons as is during thepropagation. Another way to reduce over-approximation errorsis input partition discussed separately below.Input partition. With input partition (also adopted by ReluValin  in the form of input bisection), the overall range will bethe union of range from each input sub-space. Input partitioncan signiﬁcantly reduce the accumulated over-approximationerror. Meanwhile it suppresses the number of branching neu-rons in each input sub-space, improving the scalability for bothprecise and over-approximated forward range propagation.Note that, input partition is highly parameterizable as thepartition can be done in one or more input dimensions, inthe form of bisection or more segments, and evenly or bydecision-boundary. One future research direction is to decidethe partition plan possibly depending on input sensitivityanalysis. Input partition is also highly parallelizable as thesub-spaces can be checked independently.",
            "refs": [
                11
            ]
        },
        {
            "index": 25,
            "text": "Our prototype tool is implemented in Python with multi-threaded parallel computation feature. We present our testsresults on the benchmark of pre-trained DNNs of a next-generation Airborne Collision Avoidance System for un-manned aircraft (ACAS Xu networks) open-sourced by theoriginal authors . We adapted their custom neural networkformat as per the requirements of our tool. As a ﬁrst pass,we set a hard threshold of 7200 seconds as our maximumexecution time before we declared the test “timeout (TO)”.All experiments were run on 2 Intel Xeon Silver 4114 CPUswith 40 logical cores and 32 GB memory. All experiments are10-threaded.",
            "refs": [
                7
            ]
        },
        {
            "index": 26,
            "text": "ACAS X is a family of aircraft collision avoidance systems,which is currently under development by FAA and NASA. ACAS Xu system is the version for unmanned aircraftcontrol. It is intended to receive sensor information regardingthe drone (the ownship) and any nearby intruder drones, andthen issue horizontal turning advisories aimed at preventingcollisions. Each advisory is assigned a score, with the lowestscore corresponding to the best action. The input sensor data,as illustrated in Figure 2, are:",
            "refs": []
        },
        {
            "index": 27,
            "text": "Fig. 2: Geometry for ACAS Xu Horizontal Logic Table. Figuretaken from .",
            "refs": [
                7
            ]
        },
        {
            "index": 28,
            "text": "We have developed a proof-of-concept prototype imple-mentation of our QE-based forward range propagation andveriﬁcation framework, with on-the-ﬂy network pruning usingpropagated neuron ranges. Partials of the proposed over-approximation and input partition methods are used for pre-liminary tests.",
            "refs": []
        },
        {
            "index": 29,
            "text": "ACAS Xu networks  are the DNN re-implementationof the ACAS Xu system, with the intention to reduce memoryconstraint of the on-board avionics hardware . In oneof the recent versions, the input state space of the ACAS Xusystem is partitioned into 45 sub-spaces by the combinationof aprev (5 values) and discretized τ (9 values) . A uniqueDNN is trained for each sub-space with the remaining 5inputs. The complete ACAS Xu networks contains an array",
            "refs": [
                15,
                7
            ]
        },
        {
            "index": 30,
            "text": "Note: All sample points are in format [δ, θ, ψ, vown, vint]. All times are in seconds. “TO” denotes timeout. value in the “precise” column denotes if theprecise range is propagated or the over-approximation otherwise.",
            "refs": []
        },
        {
            "index": 31,
            "text": "of 45 such fully-connected DNNs, each of which (denoted asNi∈,j∈) has total 300 hidden neurons evenly distributedin 6 hidden layers. Each hidden neuron consists of a linearweighted sum function feeding to a ReLU activation function.Recall that, in this paper, we denote the output range (resp.value) of the weight sum node of a neuron as its z-range(resp. z-value), and the output range (resp. value) of theactivation node as its a-range (resp. a-value). The output layerof each DNN consists of ﬁve output neurons computing andoutputting only weighted sum of the a-values of neurons inthe previous layer, i.e, the output neurons do not possessactivation function. The ﬁve output neurons are denoted byQCOC, QW L, QW R, QSL, and QSR respectively, one for eachof the ﬁve possible advisories: Clear-of-Conﬂict (COC), weakleft (WL), weak right (WR), strong left (SL), and strongright (SR). An advisory is selected as the DNN output if thecorresponding output neuron has the minimal z-value.",
            "refs": [
                5,
                9
            ]
        },
        {
            "index": 32,
            "text": "In this section, we ﬁrst present our tests results regardinglocal adversarial robustness deﬁned by Def .1 in Section II-B.Then, we introduce our new feature of precise quantitativeanalysis of the local robustness region regarding Def. 2 inSection II-B. Finally, we show from one test case that ourapproach has the potential to be used for more general typeof DNN input-output property veriﬁcation.",
            "refs": []
        },
        {
            "index": 33,
            "text": "δ-Local-Robustness Tests: δ-local-robustness requires a δ-neighborhood of a sample point agrees on the output label.This is the human basis for identifying mis-classiﬁcation. Wetested on one of the 45 networks, with the same 5 (normalized)sample points by Reluplex  and 3 perturbation values on thenormalized inputs. After forward range propagation, one cancompare the range bounds on each output neuron to decide ifone of the labels always corresponds to the least value. Table. Ishows this veriﬁcation results based naively on output ranges.A few comments on the δ-local-robustness tests results and",
            "refs": [
                7
            ]
        },
        {
            "index": 34,
            "text": "• When δ increases to 0.01, most precise range propagationcauses timeout due to the increasing number of branchingneurons. But veriﬁcation on 4 out 5 sample points can stillbe completed conclusively using over-approximation.",
            "refs": []
        },
        {
            "index": 35,
            "text": "is “UNKNOWN” using naiverange comparison among output neurons when there isoverlapping on those ranges. Input partition (not yet usedfor this group of tests) can be adopted to reﬁne the rangebounds on input sub-space for decision, and/or the rangecomputation formulation can be augmented with extralinear constraints of encoding the guarantee on the correctlabel getting the minimum value.",
            "refs": []
        },
        {
            "index": 36,
            "text": "• Our timeout threshold is relatively low of 7200 seconds.Some of the “TOs” in Table. I actually get results back inlonger time. For example, precise forward range propa-gation of P5 with δ = 0.01 on DNN N1,1 is completed in16216 seconds. Since its computation time at δ = 0.005(75 seconds) is drastically lower, it suggests that bisectioncould improve the case. We will verify that in the nearfuture.",
            "refs": []
        },
        {
            "index": 37,
            "text": "(δ, \u0001)-Local-Robustness Tests: (δ, \u0001)-local-robustness empha-sizes the \u0001-smoothness in the continuous output space (insteadof discrete labels) within the δ-neighborhood of a samplepoint. The key challenge here is to ﬁnd the precise mappingbetween the two parameters δ and \u0001 in dual directions. To thebest of our knowledge, our prototype is the ﬁrst to achievesuch precise mappings at the presence of branching neurons.Without loss of generality, we demonstrate our approach onthe COC output neuron only.",
            "refs": []
        },
        {
            "index": 38,
            "text": "1) δ-to-\u0001 precise mapping: This precise mapping is a natu-ral result of our QE-based precise forward range propagationgiven a small δ. For example, the precise range of the QCOCoutput of DNN N1,1 with in the 0.01-neighborhood of P3in Table I, is computed to be (rounded to the nearest 10−8). The largest absolute differencefrom the sample output of P3 (−0.02157623) is the precise \u0001(=0.00000651).",
            "refs": []
        },
        {
            "index": 39,
            "text": "2) \u0001-to-δ precise mapping: Narrowing down the δ for agiven \u0001 helps isolating adversarial ranges of inputs from thenon-adversarial ones. It allows a DNN developer to do so po-tentially with a desired precision. Both Reluplex and ReluValuse iterative bisections on the original δ to approach a betterprecision. Our approach takes this to a further level, allowingdirect backward derivation of the precise input perturbationfor a desired \u0001.",
            "refs": []
        },
        {
            "index": 40,
            "text": "Given (cid:126)x0 as the sample input and an original reference δ0,we ﬁrst perform precise forward range propagation to obtainthe corresponding \u00010 along with a simpliﬁed behavioralstructure. Then for a desired output deviation \u0001∗ < \u00010, weare able to formulate the derivation of the correspondingprecise input perturbation δ∗ using QE in a mutated versionof (1) with δ as integrated variable and all the other variablesproperly quantiﬁed. After the QE procedure eliminates allthe quantiﬁers and the quantiﬁed variables, its quantiﬁer-freeequivalence is a simple predicate with δ as the only remainingvariable. Taking again DNN N1,1 and P3 as an example, ifwe pick the desired \u0001∗ = 0.000003 (< 0.00000651), i.e.,\u0001∗ is less than the deviation under the original δ0 = 0.01,then δ ≤ 0.00460093247 is derived in a single step. Hencethe precise δ∗ is 0.00460093247. The time cost for thiscomputation is only 4.5 seconds.",
            "refs": []
        },
        {
            "index": 41,
            "text": "Remark: Before the derivation of δ∗, a forward range prop-agation with an original δ0 is necessary to obtain the muchsimpler network behavioral structure. Otherwise QE solvingusing the encoding of the entire network is too computationalcostly to be tractable. Nevertheless, the forward range prop-agation can be relaxed to allow over-approximation in somecases. When the \u00010 computed from δ0 is an over-approximatedvalue, then the precise δ∗ derived from the desired \u0001∗ < \u00010is valid if and only if δ∗ ≤ δ0. Otherwise, the network is notguaranteed to behave under the behavioral structure of δ0,thus the derivation loses soundness.A General Property Test. In addition to local robustness testspresented above, we tested our prototype on one of the originalproperties, namely, φ1 described in Reluplex paper . φ1 hasa much larger input space comparing to all the local robustnesstest cases, and it speciﬁes that the COC output value neverexceeds 1500. Our prototype is still under reﬁnement, we areable to test φ1 on DNN N1,1, mainly to evaluate the potentialperformance improvement with over-approximation and inputpartition for veriﬁcation problems. Using input bisection on allﬁve input dimensions, the union of the COC output range of all32 sub-spaces is  (roundedto the nearest 10−8). Thus φ1 is proven to be valid. Theaverage time for each input sub-space range propagation is82 seconds. If our prototype has similar performance on therest of the 45 ACAS Xu networks, and computation resourcesallow parallel computation of all input sub-spaces, it will be adrastic performance elevation (assumed 2,624 seconds on all45 networks) comparing to Reluplex (>443,560.73 seconds)and ReluVal (14,603.27 seconds).",
            "refs": [
                7
            ]
        },
        {
            "index": 42,
            "text": "Conclusion. In the paper, we presented our contributiontowards bridging the gap between the advances in DNNapplications and the rigorous veriﬁcation needs arising fromsafety concerns,in the form of a prototype tool with aproof-of-concept implementation. Our QE-based approach ap-plies to ReLU-activated DNNs. The preliminary tests re-sults are promising and show the potential of state-of-the-art performance with further reﬁned implementation of over-approximation and input partition techniques. A unique featureof our approach is that it is able to forward propagate theneurons ranges precisely at the existence of a few branchingneurons, allowing a more ﬁne-grained quantitative adversarialanalysis. A second unique feature is its ability to derive theprecise input perturbation from a desired output deviation,assisting the DNN developer to isolate and avoid adversarialranges with a given precision.Future Work. We plan to focus on 1) reﬁning our over-approximation and input partition algorithms, 2) customizingQE algorithms/solvers to achieve better performance on theclasses of QE problems formulated in our approach, and 3)conducting thorough tests to get a comprehensive evaluationand comparison with state-of-the-art tools such as Reluplexand ReluVal. We envision, in the long term, QE-based rangepropagation can be integrated with other formal veriﬁcationprocedures, to achieve more ﬂexibility and efﬁciency on abroader range of DNN formal analysis.",
            "refs": []
        },
        {
            "index": 43,
            "text": " C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Er-han, I. J. Goodfellow, and R. Fergus, “Intriguing prop-erties of neural networks,” CoRR, vol. abs/1312.6199,2013.",
            "refs": []
        },
        {
            "index": 44,
            "text": " X. Huang, D. Kroening, M. Kwiatkowska, W. Ruan,Y. Sun, E. Thamo, M. Wu, and X. Yi, “Safety andtrustworthiness of deep neural networks: A survey,” arXivpreprint arXiv:1812.08342, 2018.",
            "refs": []
        },
        {
            "index": 45,
            "text": " E. Alves, D. Bhatt, B. Hall, K. Driscoll, and A. Muruge-san, “Considerations in assuring safety of increasinglyautonomous systems,” tech. rep., NASA - Langley Re-search Center, 2018.",
            "refs": [
                3
            ]
        },
        {
            "index": 46,
            "text": " N. Akhtar and A. Mian, “Threat of adversarial attackson deep learning in computer vision: A survey,” IEEEAccess, vol. 6, pp. 14410–14430, 2018.",
            "refs": [
                4
            ]
        },
        {
            "index": 47,
            "text": " J. Su, D. V. Vargas, and K. Sakurai, “One pixel attackfor fooling deep neural networks,” IEEE Transactions onEvolutionary Computation, 2019.",
            "refs": [
                5
            ]
        },
        {
            "index": 48,
            "text": " X. Yuan, P. He, Q. Zhu, and X. Li, “Adversarial ex-amples: Attacks and defenses for deep learning,” IEEEtransactions on neural networks and learning systems,2019.",
            "refs": [
                6
            ]
        },
        {
            "index": 49,
            "text": " G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J.Kochenderfer, “Reluplex: An efﬁcient smt solver for veri-fying deep neural networks,” in International Conferenceon Computer Aided Veriﬁcation, pp. 97–117, Springer,2017.",
            "refs": [
                9
            ]
        },
        {
            "index": 50,
            "text": " D. Gopinath, G. Katz, C. S. Pasareanu, and C. Bar-rett, “Deepsafe: A data-driven approach for checkingadversarial robustness in neural networks,” arXiv preprintarXiv:1710.00486, 2017.",
            "refs": []
        },
        {
            "index": 51,
            "text": " S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana,“Formal security analysis of neural networks using sym-bolic intervals,” in 27th {USENIX} Security Symposium({USENIX} Security 18), pp. 1599–1614, 2018.",
            "refs": [
                11
            ]
        },
        {
            "index": 52,
            "text": " H. Ren, M. Clark, and R. Kumar, “Integration of quan-tiﬁer eliminator with model checker and compositionalreasoner,” in 2018 IEEE 14th International Conferenceon Control and Automation (ICCA), pp. 1070–1075,IEEE, 2018.",
            "refs": []
        },
        {
            "index": 53,
            "text": " H. Ren, R. Kumar, and M. Clark, “ReLIC: Reducedlogic inference for composition for quantiﬁer eliminationbased compositional reasoning,” in 16th InternationalConference on Informatics in Control, Automation andRobotics (ICINCO), Springer, 2019.",
            "refs": [
                13
            ]
        },
        {
            "index": 54,
            "text": " A. Dolzmann and T. Sturm, “Redlog: Computer algebrameets computer logic,” Acm Sigsam Bulletin, vol. 31,no. 2, pp. 2–9, 1997.",
            "refs": []
        },
        {
            "index": 55,
            "text": " K. D. Julian, J. Lopez, J. S. Brush, M. P. Owen, and M. J.Kochenderfer, “Policy compression for aircraft collisionavoidance systems,” in 2016 IEEE/AIAA 35th DigitalAvionics Systems Conference (DASC), pp. 1–10, IEEE,2016.",
            "refs": [
                15
            ]
        },
        {
            "index": 56,
            "text": " O. Bastani, Y. Ioannou, L. Lampropoulos, D. Vytiniotis,A. Nori, and A. Criminisi, “Measuring neural net robust-ness with constraints,” in Advances in neural informationprocessing systems, pp. 2613–2621, 2016.",
            "refs": [
                16
            ]
        },
        {
            "index": 57,
            "text": " I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explainingand harnessing adversarial examples,” arXiv preprintarXiv:1412.6572, 2014.",
            "refs": [
                17
            ]
        },
        {
            "index": 58,
            "text": " X. Huang, M. Kwiatkowska, S. Wang, and M. Wu,“Safety veriﬁcation of deep neural networks,” in Interna-tional Conference on Computer Aided Veriﬁcation, pp. 3–29, Springer, 2017.",
            "refs": [
                18
            ]
        },
        {
            "index": 59,
            "text": " C. Liu, T. Arnon, C. Lazarus, C. Barrett, and M. J.Kochenderfer, “Algorithms for verifying deep neural net-works,” arXiv preprint arXiv:1903.06758, 2019.",
            "refs": []
        },
        {
            "index": 60,
            "text": " A. Tarski, “A decision method for elementary algebraand geometry,” in Quantiﬁer elimination and cylindricalalgebraic decomposition, pp. 24–84, Springer, 1998.",
            "refs": []
        },
        {
            "index": 61,
            "text": " G. E. Collins, “Quantiﬁer elimination for real closedﬁelds by cylindrical algebraic decompostion,” in Au-tomata Theory and Formal Languages 2nd GI Confer-ence Kaiserslautern, May 20–23, 1975, pp. 134–183,Springer, 1975.",
            "refs": [
                21
            ]
        },
        {
            "index": 62,
            "text": " “Wolfram language & system documentation.” https:",
            "refs": [
                22
            ]
        },
        {
            "index": 63,
            "text": " V. Weispfenning, “The complexity of linear problems inﬁelds,” Journal of symbolic computation, vol. 5, no. 1-2,pp. 3–27, 1988.",
            "refs": []
        },
        {
            "index": 64,
            "text": " G. E. Collins and H. Hong, “Partial cylindrical algebraicdecomposition for quantiﬁer elimination,” Journal ofSymbolic Computation, vol. 12, no. 3, pp. 299–328, 1991. D. Stewart, M. W. Whalen, D. Cofer, and M. P. Heim-dahl, “Architectural modeling and analysis for safety en-gineering,” in International Symposium on Model-BasedSafety and Assessment, pp. 97–111, Springer, 2017.",
            "refs": [
                25
            ]
        },
        {
            "index": 65,
            "text": " T. A. Henzinger, M. Minea, and V. Prabhu, “Assume-guarantee reasoning for hierarchical hybrid systems,” inInternational Workshop on Hybrid Systems: Computationand Control, pp. 275–290, Springer, 2001.",
            "refs": []
        },
        {
            "index": 66,
            "text": " R. E. Moore, R. B. Kearfott, and M. J. Cloud, Introduc-",
            "refs": []
        },
        {
            "index": 67,
            "text": " K. D. Julian, M. J. Kochenderfer, and M. P. Owen,“Deep neural network compression for aircraft collisionavoidance systems,” Journal of Guidance, Control, andDynamics, vol. 42, no. 3, pp. 598–608, 2018.",
            "refs": []
        }
    ],
    "1": [
        {
            "index": 0,
            "text": "Latest addition to the toolbox of human species is ArtiﬁcialIntelligence(AI). Thus far, AI has made signiﬁcant progress inlow stake low risk scenarios such as playing Go and we arecurrently in a transition toward medium stake scenarios suchas Visual Dialog. In my thesis, I argue that we need to incor-porate controlled de-entanglement as ﬁrst class object to suc-ceed in this transition.I present mathematical analysis frominformation theory to show that employing stochasticity leadsto controlled de-entanglement of relevant factors of variation atvarious levels. Based on this, I highlight results from initialexperiments that depict efﬁcacy of the proposed framework. Iconclude this writeup by a roadmap of experiments that showthe applicability of this framework to scalability, ﬂexibility andinterpretibility.",
            "refs": []
        },
        {
            "index": 1,
            "text": "In order to bring technology closer to humans it is important toprovide mechanisms for them to interact with the same. Ap-plications like Speech Recognition, Speech Synthesis, VisualQuestion Answering and Machine Translation to name a fewhave been shown to be massively useful in this context. SuchLanguage Technologies are both rich in terms of quantity anddiversity of tasks as well as inherent complexity due to the pres-ence of human element. Hence they are the epitomy of mediumrisk medium stake applications forming a bridge between lowstake applications such as playing games, spam ﬁltering andhigh stake applications such as climate control, health relatedapplications. Consequently, we can employ such applicationsas sanity tests in gauging the ability of AI - the latest addi-tion to human toolbox - in fulﬁlling its potential to impact ourlives. However, there are three broad problems that challengethis promise - (1) Scalability - These technologies are currentlyonly accessible in a handful number of languages around theplanet. In order to have a meaningful impact it is imperativethat such technologies need to at least exist in many more lan-guages. (2) Flexibility - Although deep learning based systemsoutperform their shallow learning counterparts in terms of qual-ity, they still pale away in terms of ﬂexibility and controllability.(3) Interpretibility - Almost every deep learning system today isakin to a black box: we can neither interpret nor justify pre-dictions by most of these models. In this thesis, I argue thatemploying a slightly different framework of learning - one withcontrolled de-entanglement of relevant factors of variation as aﬁrst class object - has the potential to address all the three is-sues simultaneously and henceforth rapidly accelerate progressin language technologies.",
            "refs": []
        },
        {
            "index": 2,
            "text": "Let us consider a typical deep learning architecture such asAlexNet. It is characterized by a series of convolutional lay-ers (feature extraction module) followed by a pooling layer anda SoftMax layer(classiﬁcation module). Note that while I men-tion AlexNet as an example, this abstraction can be extended tomost sequence to sequence architectures with encoder as featureextraction module and decoder as the classiﬁcation moduleacross modalities and tasks. It can be shown that the poolinglayer acts as information bottleneck module in such architec-tures. I point out that in case of conventional Seq2Seq archi-tectures deployed today, attention plays the role of informationbottleneck module regulating the amount of information beingutilized by the decoder. In  I show that this module con-trols optimization in encoder decoder models leading to (1) Dis-entanglement of Causal Factors of variation in the data distribu-tion (2) Marginalization of nuisance factors of variation fromthe input distribution. In case of models that employ stochastic-ity, two more effects can be observed : (a) Posterior collapse orDegeneration due to powerful decoders and (b) Loss of outputﬁdelity due to ﬁnite capacity decoders. In current architectures,marginalization and disentanglement are realized implicitly andoften lead to (a) and (b) when deployed in practise. I posit thatdesigning learning paradigms such that we explicitly control de-entanglement of relevant factors of variation while marginaliz-ing the nuisance factors of variation leads to massive improve-ments. I refer to this as controlled DeEntanglement. Such anapproach, I claim, leads to further advantages in the context ofboth generative processes - generation of novel content and dis-criminative processes - robustness of such models to noise andattacks.",
            "refs": [
                1,
                2,
                5
            ]
        },
        {
            "index": 3,
            "text": "The most popular approach to obtain disentanglement in neu-ral models is by employing stochastic random variables. Thisapproach provides ﬂexibility to jointly train the latent represen-tations as well as the downstream network. It has been observedthat the latent representations resemble disentangled represen-tations under certain conditions . Note that althoughobtaining such degenerate representations is typical, it is not theonly manifestation of disentanglement: it also manifests as con-tinuous representations and other abstract phenomena(e.g.grounding).I argue that explicitly controlling what and howmuch gets de-entangled  is better than implicit disentangle-ment as is followed today. I identify four ways to computa-tionally control de-entanglement in encoder decoder models, byemploying (1) suitable priors (2) additional adversarial or multitask objectives (3) an alternative formulation of probability den-sity estimation and (4) a different objective of divergence. Inmy thesis, I present experimental ﬁndings from various tasks to",
            "refs": [
                6,
                7,
                9,
                11
            ]
        },
        {
            "index": 4,
            "text": "show that the proposed framework has the ability to address allthe three problems mentioned in section 1: scalability via priorsbased controlled de-entanglement, ﬂexibility via priors, adver-sarial training and interpretibility via alternative formulation ofdensity estimation and objective of divergence.",
            "refs": []
        },
        {
            "index": 5,
            "text": "A major bottleneck in the progress of many data-intensive lan-guage processing tasks such as speech recognition and synthe-sis is scalability to new languages and domains. Building suchtechnologies for unwritten or under-resourced languages is of-ten not feasible due to lack of annotated data or other expensiveresources. I posit that there are two distinct categorizations thatpose challenges in terms of scalability: (1) Unwritten languagesand low resource scenarios and (2) Code switching and othernon native speech phenomena.",
            "refs": []
        },
        {
            "index": 6,
            "text": "Let us consider building speech technology for unwritten orunder-resourced languages. A fundamental resource requiredto build speech technology stack in such languages is pho-netic lexicon: something that translates acoustic input to tex-tual representation. Having such a lexicon - even if noisy andincomplete - can help bootstrap speech recognition and synthe-sis models which in turn enable other applications such as keyword spotting. Therefore I proposed to employ controlled de-entanglement for unsupervised acoustic unit discovery in thecontext of our submission to ZeroSpeech Challenge 2019 .We make an observation that articulatory information aboutspeech production presents a discrete set of independent con-straints. For instance, manner and place of articulation are twoarticulatory dimensions characterized by discrete sets(labial vsdental, etc). Based on this, we condition the prior space to con-form to articulatory conditions by using a bank of discrete priordistributions.",
            "refs": []
        },
        {
            "index": 7,
            "text": "Speech has both continuous as well as discrete priors: The gen-erative process of speech assumes a Gaussian prior distributionwhich is continuous in nature. However, the language whichis also present in the utterance can be approximated to be sam-pled from a discrete prior distribution. Exact manifestation ofthis in linguistics can be at different levels: phonemes, words,syllables, sub word units, etc. Based on this insight, I showthat incorporating priors help encode language independent in-formation thereby facilitating synthesis of code mixed content.In addition to basing priors on knowledge about characteristics,I believe that it is also possible to base them on discovered pat-terns. In , we have discovered several code switching stylesbased on . I plan to incorporate priors based on this styleinformation to help speech recognition models targeted at de-coding code switched speech.",
            "refs": [
                13,
                14
            ]
        },
        {
            "index": 8,
            "text": "We humans exhibit explicit global as well as ﬁne grained con-trol over how we deliver information. This enables us commu-nicate more effectively in a conversation. The goal is to build AImodels that can mimic this behavior. I have thus far worked onimage captioning in the context of global control and emphatictext to speech in the context of ﬁne grained control.",
            "refs": []
        },
        {
            "index": 9,
            "text": "In the context of image captioning, an interesting observationis that both the involved modalities - textual even though pri-marily symbolic and visual even though primarily spatial - arecharacterized by distinct discrete and continuous factors of vari-ation. For instance, distinct objects or entities would intuitivelyperhaps be better represented by discrete variables, while theirspatial location and relationships between them might be rep-resented by continuous variables. Therefore, we split the la-tent prior space used for approximating the posterior distri-bution into continuous and discrete counterparts. Pressurizingthe model to encode such prior information into the latent spaceprovides us the ﬂexibility to control the generative process bypinging different latent states during inference.",
            "refs": [
                5
            ]
        },
        {
            "index": 10,
            "text": "I am interested in investigating approaches to incorporate au-tomatically derivable information from speech into the modelarchitecture for better modeling and controlling prosody. In asample implementation1, I have attempted disentangling heuris-tics about tonal information to accomplish local control. I ﬁrstquantize fundamental frequency(F0) - a feature highly corre-lated with prosody - into multiple bins. I then incorporate theresultant discretized ordinal information at the phone level intoour speech generation architecture in two different scenarios:(1) Explicit labeling scenario where we input the quantized F0values alongside the phoneme embeddings as input to the en-coder and (2) Implicit incorporation as an additional task topredict these quantized F0s at the output of encoder. The modelwas optimized using ordinal triplet divergence . We showthat our approach generates appropriate emphasis at word leveland signiﬁcantly outperforms AuToBI in terms of ﬂexibility.",
            "refs": []
        },
        {
            "index": 11,
            "text": "Language is inherently composite in nature. Hence it is vitalto model the abstract relationships so as to capture the unseencompositions of seen concepts at test time. However, accom-plishing this is a deceptively non trivial task and might lead tomodels learning just surface level associations.Ipresent a case that ﬂexible generative models provide additionalinformation to improve performance in such tasks. Further, Ihypothesize that when optimized using either a disjoint learn-ing mechanism or a different divergence function, such mod-els can also act as justifying modules for the task at hand. Toground this argument, I present two example applications thatemploy ﬂexible models mentioned in the previous subsections:(1) Visual Question Answering System(VQA) that receivers ad-ditional information in the form of targeted captions. I proposeto use Reward Augmented Maximum Likelihood to gener-ate and integrate captions in the framework of Visual QuestionAnswering. I show that tying the reward function to length ofthe generated caption forces the model to encode most relevantinformation thereby acting as justiﬁcation to the selected an-swer. (2) Based on similar insights, I propose to apply toobtain and integrate speech recognition transcripts in the con-text of Acoustic Topic Identiﬁcation System.",
            "refs": [
                16,
                17,
                18
            ]
        },
        {
            "index": 12,
            "text": "and student volunteers for taking part in the various subjectiveevaluations. I am very grateful to my collaborators for provid-ing me an opportunity to work with them and the reviewers foruseful feedback at various stages in my research career.",
            "refs": []
        },
        {
            "index": 13,
            "text": " M. Norouzi, S. Bengio, N. Jaitly, M. Schuster, Y. Wu, D. Schuur-mans et al., “Reward augmented maximum likelihood for neuralstructured prediction,” in Advances In Neural Information Pro-cessing Systems, 2016.",
            "refs": []
        },
        {
            "index": 14,
            "text": " A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classi-ﬁcation with deep convolutional neural networks,” in Advances inneural information processing systems, 2012.",
            "refs": [
                1
            ]
        },
        {
            "index": 15,
            "text": " D. Rousseau and S. Tsaftaris, “Data augmentation techniques fordeep learning,” in Tutorial Session, IEEE International Confer-ence on Acoustics, Speech and Signal Processing (ICASSP), 2019.",
            "refs": [
                2
            ]
        },
        {
            "index": 16,
            "text": " N. Tishby, F. C. Pereira, and W. Bialek, “The information bottle-",
            "refs": []
        },
        {
            "index": 17,
            "text": " S. K. Rallabandi and A. Black, “Variational Attention using Ar-ticulatory priors for generating code mixed speech using mono-lingual corpora,” in in proceedings of Interspeech, 2019.",
            "refs": []
        },
        {
            "index": 18,
            "text": " N. Vyas, S. Rallabandi, L. Morishetti, E. Hovy, and A. W. Black,“Learning disentangled representation in latent stochastic models:A case study with image captioning,” in IEEE International Con-ference on Acoustics, Speech and Signal Processing (ICASSP).IEEE, 2019.",
            "refs": [
                5
            ]
        },
        {
            "index": 19,
            "text": " T. Q. Chen, X. Li, R. B. Grosse, and D. K. Duvenaud, “Isolatingsources of disentanglement in variational autoencoders,” in Ad-vances in Neural Information Processing Systems, 2018.",
            "refs": [
                6
            ]
        },
        {
            "index": 20,
            "text": " C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Des-jardins, and A. Lerchner, “Understanding disentangling in BetaVAE,” arXiv preprint arXiv:1804.03599, 2018.",
            "refs": [
                7
            ]
        },
        {
            "index": 21,
            "text": " B. Esmaeili, H. Wu, S. Jain, A. Bozkurt, N. Siddharth, B. Paige,D. H. Brooks, J. Dy, and J.-W. van de Meent, “Structured disen-tangled representations,” in International Conference on ArtiﬁcialIntelligence and Statistics, 2018.",
            "refs": []
        },
        {
            "index": 22,
            "text": " M. Ravanelli and Y. Bengio, “Interpretable convolutional ﬁlters",
            "refs": []
        },
        {
            "index": 23,
            "text": " F. Locatello, S. Bauer, M. Lucic, S. Gelly, B. Sch¨olkopf, andO. Bachem, “Challenging common assumptions in the unsuper-vised learning of disentangled representations,” arXiv preprintarXiv:1811.12359, 2018.",
            "refs": [
                11
            ]
        },
        {
            "index": 24,
            "text": " S. Rallabandi and A. Black, “Submission from CMU to Ze-",
            "refs": []
        },
        {
            "index": 25,
            "text": " S. Rallabandi, S. Sitaram, and A. W. Black, “Automatic detec-tion of code-switching style from acoustics,” in Proceedings ofthe Third Workshop on Computational Approaches to LinguisticCode-Switching, 2018.",
            "refs": [
                13
            ]
        },
        {
            "index": 26,
            "text": " G. A. Guzm´an, J. Ricard, J. Serigos, B. E. Bullock, and A. J.Toribio, “Metrics for modeling code-switching across corpora.”in Proceedings of INTERSPEECH, 2017.",
            "refs": [
                14
            ]
        },
        {
            "index": 27,
            "text": " P. Wu, S. Rallabandi, E. Nyberg, and A. Black, “Ordinal tripletloss: Investigating sleepiness detection from speech,” in Inter-speech, 2019.",
            "refs": []
        },
        {
            "index": 28,
            "text": " A. Agrawal, A. Kembhavi, D. Batra, and D. Parikh, “C-vqa: Acompositional split of the visual question answering (vqa) v1. 0dataset,” arXiv preprint arXiv:1704.08243, 2017.",
            "refs": [
                16
            ]
        },
        {
            "index": 29,
            "text": " H. Chen, H. Zhang, P.-Y. Chen, J. Yi, and C.-J. Hsieh, “At-tacking visual language grounding with adversarial examples:A case study on neuralimage captioning,” arXiv preprintarXiv:1712.02051, 2017.",
            "refs": [
                17
            ]
        },
        {
            "index": 30,
            "text": " Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh,“Making the v in vqa matter: Elevating the role of image un-derstanding in visual question answering,” in Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition,2017.",
            "refs": [
                18
            ]
        }
    ],
    "2": [
        {
            "index": 0,
            "text": "The recently introduced continuous Skip-gram model is an efﬁcient method forlearning high-quality distributed vector representations that capture a large num-ber of precise syntactic and semantic word relationships. In this paper we presentseveral extensions that improve both the quality of the vectors and the trainingspeed. By subsampling of the frequent words we obtain signiﬁcant speedup andalso learn more regular word representations. We also describe a simple alterna-tive to the hierarchical softmax called negative sampling.An inherent limitation of word representations is their indifference to word orderand their inability to represent idiomatic phrases. For example, the meanings of“Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivatedby this example, we present a simple method for ﬁnding phrases in text, and showthat learning good vector representations for millions of phrases is possible.",
            "refs": []
        },
        {
            "index": 1,
            "text": "Distributed representations of words in a vector space help learning algorithms to achieve betterperformance in natural language processing tasks by grouping similar words. One of the earliest useof word representations dates back to 1986 due to Rumelhart, Hinton, and Williams . This ideahas since been applied to statistical language modeling with considerable success . The followup work includes applications to automatic speech recognition and machine translation , anda wide range of NLP tasks .",
            "refs": [
                13,
                1,
                14,
                7,
                2,
                15,
                3,
                18,
                19,
                9
            ]
        },
        {
            "index": 2,
            "text": "Recently, Mikolov et al.  introduced the Skip-gram model, an efﬁcient method for learning high-quality vector representations of words from large amounts of unstructured text data. Unlike mostof the previously used neural network architectures for learning word vectors, training of the Skip-gram model (see Figure 1) does not involve dense matrix multiplications. This makes the trainingextremely efﬁcient: an optimized single-machine implementation can train on more than 100 billionwords in one day.",
            "refs": [
                8
            ]
        },
        {
            "index": 3,
            "text": "The word representations computed using neural networks are very interesting because the learnedvectors explicitly encode many linguistic regularities and patterns. Somewhat surprisingly, many ofthese patterns can be represented as linear translations. For example, the result of a vector calcula-tion vec(“Madrid”) - vec(“Spain”) + vec(“France”) is closer to vec(“Paris”) than to any other wordvector .",
            "refs": [
                9,
                8
            ]
        },
        {
            "index": 4,
            "text": "In this paper we present several extensions of the original Skip-gram model. We show that sub-sampling of frequent words during training results in a signiﬁcant speedup (around 2x - 10x), andimproves accuracy of the representations of less frequent words. In addition, we present a simpli-ﬁed variant of Noise Contrastive Estimation (NCE)  for training the Skip-gram model that resultsin faster training and better vector representations for frequent words, compared to more complexhierarchical softmax that was used in the prior work .",
            "refs": [
                4,
                8
            ]
        },
        {
            "index": 5,
            "text": "Word representations are limited by their inability to represent idiomatic phrases that are not com-positions of the individual words. For example, “Boston Globe” is a newspaper, and so it is not anatural combination of the meanings of “Boston” and “Globe”. Therefore, using vectors to repre-sent the whole phrases makes the Skip-gram model considerably more expressive. Other techniquesthat aim to represent meaning of sentences by composing the word vectors, such as the recursiveautoencoders , would also beneﬁt from using phrase vectors instead of the word vectors.",
            "refs": [
                15
            ]
        },
        {
            "index": 6,
            "text": "The extension from word based to phrase based models is relatively simple. First we identify a largenumber of phrases using a data-driven approach, and then we treat the phrases as individual tokensduring the training. To evaluate the quality of the phrase vectors, we developed a test set of analogi-cal reasoning tasks that contains both words and phrases. A typical analogy pair from our test set is“Montreal”:“Montreal Canadiens”::“Toronto”:“Toronto Maple Leafs”. It is considered to have beenanswered correctly if the nearest representation to vec(“Montreal Canadiens”) - vec(“Montreal”) +vec(“Toronto”) is vec(“Toronto Maple Leafs”).",
            "refs": []
        },
        {
            "index": 7,
            "text": "Finally, we describe another interesting property of the Skip-gram model. We found that simplevector addition can often produce meaningful results. For example, vec(“Russia”) + vec(“river”) isclose to vec(“Volga River”), and vec(“Germany”) + vec(“capital”) is close to vec(“Berlin”). Thiscompositionality suggests that a non-obvious degree of language understanding can be obtained byusing basic mathematical operations on the word vector representations.",
            "refs": []
        },
        {
            "index": 8,
            "text": "where vw and v′w are the “input” and “output” vector representations of w, and W is the num-ber of words in the vocabulary. This formulation is impractical because the cost of computing∇ log p(wO|wI ) is proportional to W , which is often large (105–107 terms).",
            "refs": []
        },
        {
            "index": 9,
            "text": "A computationally efﬁcient approximation of the full softmax is the hierarchical softmax. In thecontext of neural network language models, it was ﬁrst introduced by Morin and Bengio . Themain advantage is that instead of evaluating W output nodes in the neural network to obtain theprobability distribution, it is needed to evaluate only about log2(W ) nodes.The hierarchical softmax uses a binary tree representation of the output layer with the W words asits leaves and, for each node, explicitly represents the relative probabilities of its child nodes. Thesedeﬁne a random walk that assigns probabilities to words.",
            "refs": [
                12
            ]
        },
        {
            "index": 10,
            "text": "w=1 p(w|wI ) = 1. This implies that thecost of computing log p(wO|wI ) and ∇ log p(wO|wI ) is proportional to L(wO), which on averageis no greater than log W . Also, unlike the standard softmax formulation of the Skip-gram whichw to each word w, the hierarchical softmax formulation hasassigns two representations vw and v′one representation vw for each word w and one representation v′n for every inner node n of thebinary tree.",
            "refs": []
        },
        {
            "index": 11,
            "text": "The structure of the tree used by the hierarchical softmax has a considerable effect on the perfor-mance. Mnih and Hinton explored a number of methods for constructing the tree structure and theeffect on both the training time and the resulting model accuracy . In our work we use a binaryHuffman tree, as it assigns short codes to the frequent words which results in fast training. It hasbeen observed before that grouping words together by their frequency works well as a very simplespeedup technique for the neural network based language models .",
            "refs": [
                10,
                5,
                8
            ]
        },
        {
            "index": 12,
            "text": "An alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE), which was in-troduced by Gutmann and Hyvarinen  and applied to language modeling by Mnih and Teh .NCE posits that a good model should be able to differentiate data from noise by means of logisticregression. This is similar to hinge loss used by Collobert and Weston  who trained the modelsby ranking the data above noise.",
            "refs": [
                4,
                11,
                2
            ]
        },
        {
            "index": 13,
            "text": "Figure 2: Two-dimensional PCA projection of the 1000-dimensional Skip-gram vectors of countries and theircapital cities. The ﬁgure illustrates ability of the model to automatically organize concepts and learn implicitlythe relationships between them, as during the training we did not provide any supervised information aboutwhat a capital city means.",
            "refs": []
        },
        {
            "index": 14,
            "text": "which is used to replace every log P (wO|wI ) term in the Skip-gram objective. Thus the task is todistinguish the target word wO from draws from the noise distribution Pn(w) using logistic regres-sion, where there are k negative samples for each data sample. Our experiments indicate that valuesof k in the range 5–20 are useful for small training datasets, while for large datasets the k can be assmall as 2–5. The main difference between the Negative sampling and NCE is that NCE needs bothsamples and the numerical probabilities of the noise distribution, while Negative sampling uses onlysamples. And while NCE approximately maximizes the log probability of the softmax, this propertyis not important for our application.Both NCE and NEG have the noise distribution Pn(w) as a free parameter. We investigated a numberof choices for Pn(w) and found that the unigram distribution U (w) raised to the 3/4rd power (i.e.,U (w)3/4/Z) outperformed signiﬁcantly the unigram and the uniform distributions, for both NCEand NEG on every task we tried including language modeling (not reported here).",
            "refs": []
        },
        {
            "index": 15,
            "text": "In very large corpora, the most frequent words can easily occur hundreds of millions of times (e.g.,“in”, “the”, and “a”). Such words usually provide less information value than the rare words. Forexample, while the Skip-gram model beneﬁts from observing the co-occurrences of “France” and“Paris”, it beneﬁts much less from observing the frequent co-occurrences of “France” and “the”, asnearly every word co-occurs frequently within a sentence with “the”. This idea can also be appliedin the opposite direction; the vector representations of frequent words do not change signiﬁcantlyafter training on several million examples.",
            "refs": []
        },
        {
            "index": 16,
            "text": "Table 1: Accuracy of various Skip-gram 300-dimensional models on the analogical reasoning taskas deﬁned in . NEG-k stands for Negative Sampling with k negative samples for each positivesample; NCE stands for Noise Contrastive Estimation and HS-Huffman stands for the HierarchicalSoftmax with the frequency-based Huffman codes.",
            "refs": [
                8
            ]
        },
        {
            "index": 17,
            "text": "where f (wi) is the frequency of word wi and t is a chosen threshold, typically around 10−5.We chose this subsampling formula because it aggressively subsamples words whose frequencyis greater than t while preserving the ranking of the frequencies. Although this subsampling for-mula was chosen heuristically, we found it to work well in practice. It accelerates learning and evensigniﬁcantly improves the accuracy of the learned vectors of the rare words, as will be shown in thefollowing sections.",
            "refs": []
        },
        {
            "index": 18,
            "text": "In this section we evaluate the Hierarchical Softmax (HS), Noise Contrastive Estimation, NegativeSampling, and subsampling of the training words. We used the analogical reasoning task1 introducedby Mikolov et al. . The task consists of analogies such as “Germany” : “Berlin” :: “France” : ?,which are solved by ﬁnding a vector x such that vec(x) is closest to vec(“Berlin”) - vec(“Germany”)+ vec(“France”) according to the cosine distance (we discard the input words from the search). Thisspeciﬁc example is considered to have been answered correctly if x is “Paris”. The task has twobroad categories: the syntactic analogies (such as “quick” : “quickly” :: “slow” : “slowly”) and thesemantic analogies, such as the country to capital city relationship.",
            "refs": [
                8
            ]
        },
        {
            "index": 19,
            "text": "For training the Skip-gram models, we have used a large dataset consisting of various news articles(an internal Google dataset with one billion words). We discarded from the vocabulary all wordsthat occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K.The performance of various Skip-gram models on the word analogy test set is reported in Table 1.The table shows that Negative Sampling outperforms the Hierarchical Softmax on the analogicalreasoning task, and has even slightly better performance than the Noise Contrastive Estimation. Thesubsampling of the frequent words improves the training speed several times and makes the wordrepresentations signiﬁcantly more accurate.",
            "refs": []
        },
        {
            "index": 20,
            "text": "It can be argued that the linearity of the skip-gram model makes its vectors more suitable for suchlinear analogical reasoning, but the results of Mikolov et al.  also show that the vectors learnedby the standard sigmoidal recurrent neural networks (which are highly non-linear) improve on thistask signiﬁcantly as the amount of the training data increases, suggesting that non-linear models alsohave a preference for a linear structure of the word representations.",
            "refs": [
                8
            ]
        },
        {
            "index": 21,
            "text": "As discussed earlier, many phrases have a meaning that is not a simple composition of the mean-ings of its individual words. To learn vector representation for phrases, we ﬁrst ﬁnd words thatappear frequently together, and infrequently in other contexts. For example, “New York Times” and“Toronto Maple Leafs” are replaced by unique tokens in the training data, while a bigram “this is”will remain unchanged.",
            "refs": []
        },
        {
            "index": 22,
            "text": "Table 2: Examples of the analogical reasoning task for phrases (the full test set has 3218 examples).The goal is to compute the fourth phrase using the ﬁrst three. Our best model achieved an accuracyof 72% on this dataset.",
            "refs": []
        },
        {
            "index": 23,
            "text": "The δ is used as a discounting coefﬁcient and prevents too many phrases consisting of very infre-quent words to be formed. The bigrams with score above the chosen threshold are then used asphrases. Typically, we run 2-4 passes over the training data with decreasing threshold value, allow-ing longer phrases that consists of several words to be formed. We evaluate the quality of the phraserepresentations using a new analogical reasoning task that involves phrases. Table 2 shows examplesof the ﬁve categories of analogies used in this task. This dataset is publicly available on the web2.",
            "refs": []
        },
        {
            "index": 24,
            "text": "Starting with the same news data as in the previous experiments, we ﬁrst constructed the phrasebased training corpus and then we trained several Skip-gram models using different hyper-parameters. As before, we used vector dimensionality 300 and context size 5. This setting alreadyachieves good performance on the phrase dataset, and allowed us to quickly compare the NegativeSampling and the Hierarchical Softmax, both with and without subsampling of the frequent tokens.The results are summarized in Table 3.",
            "refs": []
        },
        {
            "index": 25,
            "text": "The results show that while Negative Sampling achieves a respectable accuracy even with k = 5,using k = 15 achieves considerably better performance. Surprisingly, while we found the Hierar-chical Softmax to achieve lower performance when trained without subsampling, it became the bestperforming method when we downsampled the frequent words. This shows that the subsamplingcan result in faster training and can also improve accuracy, at least in some cases.",
            "refs": []
        },
        {
            "index": 26,
            "text": "To maximize the accuracy on the phrase analogy task, we increased the amount of the training databy using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionalityof 1000, and the entire sentence for the context. This resulted in a model that reached an accuracyof 72%. We achieved lower accuracy 66% when we reduced the size of the training dataset to 6Bwords, which suggests that the large amount of the training data is crucial.",
            "refs": []
        },
        {
            "index": 27,
            "text": "To gain further insight into how different the representations learned by different models are, we didinspect manually the nearest neighbours of infrequent phrases using various models. In Table 4, weshow a sample of such comparison. Consistently with the previous results, it seems that the bestrepresentations of phrases are learned by a model with the hierarchical softmax and subsampling.",
            "refs": []
        },
        {
            "index": 28,
            "text": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibita linear structure that makes it possible to perform precise analogical reasoning using simple vectorarithmetics. Interestingly, we found that the Skip-gram representations exhibit another kind of linearstructure that makes it possible to meaningfully combine words by an element-wise addition of theirvector representations. This phenomenon is illustrated in Table 5.",
            "refs": []
        },
        {
            "index": 29,
            "text": "The additive property of the vectors can be explained by inspecting the training objective. The wordvectors are in a linear relationship with the inputs to the softmax nonlinearity. As the word vectorsare trained to predict the surrounding words in the sentence, the vectors can be seen as representingthe distribution of the context in which a word appears. These values are related logarithmicallyto the probabilities computed by the output layer, so the sum of two word vectors is related to theproduct of the two context distributions. The product works here as the AND function: words thatare assigned high probabilities by both word vectors will have high probability, and the other wordswill have low probability. Thus, if “Volga River” appears frequently in the same sentence togetherwith the words “Russian” and “river”, the sum of these two word vectors will result in such a featurevector that is close to the vector of “Volga River”.",
            "refs": []
        },
        {
            "index": 30,
            "text": "Many authors who previously worked on the neural network based representations of words havepublished their resulting models for further use and comparison: amongst the most well known au-thors are Collobert and Weston , Turian et al. , and Mnih and Hinton . We downloadedtheir word vectors from the web3. Mikolov et al.  have already evaluated these word representa-tions on the word analogy task, where the Skip-gram models achieved the best performance with ahuge margin.",
            "refs": [
                2,
                10,
                8
            ]
        },
        {
            "index": 31,
            "text": "Table 6: Examples of the closest tokens given various well known models and the Skip-gram modeltrained on phrases using over 30 billion training words. An empty cell means that the word was notin the vocabulary.",
            "refs": []
        },
        {
            "index": 32,
            "text": "To give more insight into the difference of the quality of the learned vectors, we provide empiricalcomparison by showing the nearest neighbours of infrequent words in Table 6. These examples showthat the big Skip-gram model trained on a large corpus visibly outperforms all the other models inthe quality of the learned representations. This can be attributed in part to the fact that this modelhas been trained on about 30 billion words, which is about two to three orders of magnitude moredata than the typical size used in the prior work. Interestingly, although the training set is muchlarger, the training time of the Skip-gram model is just a fraction of the time complexity required bythe previous model architectures.",
            "refs": []
        },
        {
            "index": 33,
            "text": "This work has several key contributions. We show how to train distributed representations of wordsand phrases with the Skip-gram model and demonstrate that these representations exhibit linearstructure that makes precise analogical reasoning possible. The techniques introduced in this papercan be used also for training the continuous bag-of-words model introduced in .",
            "refs": [
                8
            ]
        },
        {
            "index": 34,
            "text": "We successfully trained models on several orders of magnitude more data than the previously pub-lished models, thanks to the computationally efﬁcient model architecture. This results in a greatimprovement in the quality of the learned word and phrase representations, especially for the rareentities. We also found that the subsampling of the frequent words results in both faster trainingand signiﬁcantly better representations of uncommon words. Another contribution of our paper isthe Negative sampling algorithm, which is an extremely simple training method that learns accuraterepresentations especially for frequent words.",
            "refs": []
        },
        {
            "index": 35,
            "text": "The choice of the training algorithm and the hyper-parameter selection is a task speciﬁc decision,as we found that different problems have different optimal hyperparameter conﬁgurations. In ourexperiments, the most crucial decisions that affect the performance are the choice of the modelarchitecture, the size of the vectors, the subsampling rate, and the size of the training window.",
            "refs": []
        },
        {
            "index": 36,
            "text": "A very interesting result of this work is that the word vectors can be somewhat meaningfully com-bined using just simple vector addition. Another approach for learning representations of phrasespresented in this paper is to simply represent the phrases with a single token. Combination of thesetwo approaches gives a powerful yet simple way how to represent longer pieces of text, while hav-ing minimal computational complexity. Our work can thus be seen as complementary to the existingapproach that attempts to represent phrases using recursive matrix-vector operations .",
            "refs": [
                16
            ]
        },
        {
            "index": 37,
            "text": " Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language",
            "refs": [
                1
            ]
        },
        {
            "index": 38,
            "text": " Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: deep neu-ral networks with multitask learning. In Proceedings of the 25th international conference on Machinelearning, pages 160–167. ACM, 2008.",
            "refs": [
                2
            ]
        },
        {
            "index": 39,
            "text": " Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classi-",
            "refs": [
                3
            ]
        },
        {
            "index": 40,
            "text": " Michael U Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation of unnormalized statistical mod-els, with applications to natural image statistics. The Journal of Machine Learning Research, 13:307–361,2012.",
            "refs": [
                4
            ]
        },
        {
            "index": 41,
            "text": " Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Extensions ofrecurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011IEEE International Conference on, pages 5528–5531. IEEE, 2011.",
            "refs": [
                5
            ]
        },
        {
            "index": 42,
            "text": " Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky. Strategies for TrainingLarge Scale Neural Network Language Models. In Proc. Automatic Speech Recognition and Understand-ing, 2011.",
            "refs": []
        },
        {
            "index": 43,
            "text": " Tomas Mikolov. Statistical Language Models Based on Neural Networks. PhD thesis, PhD Thesis, Brno",
            "refs": [
                7
            ]
        },
        {
            "index": 44,
            "text": " Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations",
            "refs": [
                8
            ]
        },
        {
            "index": 45,
            "text": " Tomas Mikolov, Wen-tau Yih and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word",
            "refs": [
                9
            ]
        },
        {
            "index": 46,
            "text": " Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. Advances in",
            "refs": [
                10
            ]
        },
        {
            "index": 47,
            "text": " Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language",
            "refs": [
                11
            ]
        },
        {
            "index": 48,
            "text": " Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Pro-",
            "refs": [
                12
            ]
        },
        {
            "index": 49,
            "text": " David E Rumelhart, Geoffrey E Hintont, and Ronald J Williams. Learning representations by back-",
            "refs": [
                13
            ]
        },
        {
            "index": 50,
            "text": " Holger Schwenk. Continuous space language models. Computer Speech and Language, vol. 21, 2007. Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. Parsing natural scenes andnatural language with recursive neural networks. In Proceedings of the 26th International Conference onMachine Learning (ICML), volume 2, 2011.",
            "refs": [
                14,
                15
            ]
        },
        {
            "index": 51,
            "text": " Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic CompositionalityThrough Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Conference on Empirical Methodsin Natural Language Processing (EMNLP), 2012.",
            "refs": [
                16
            ]
        },
        {
            "index": 52,
            "text": " Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method forsemi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computa-tional Linguistics, pages 384–394. Association for Computational Linguistics, 2010.",
            "refs": []
        },
        {
            "index": 53,
            "text": " Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. In",
            "refs": [
                18
            ]
        },
        {
            "index": 54,
            "text": " Peter D. Turney. Distributional semantics beyond words: Supervised learning of analogy and paraphrase.",
            "refs": [
                19
            ]
        },
        {
            "index": 55,
            "text": " Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image annota-tion. In Proceedings of the Twenty-Second international joint conference on Artiﬁcial Intelligence-VolumeVolume Three, pages 2764–2770. AAAI Press, 2011.",
            "refs": []
        }
    ],
    "3": [
        {
            "index": 0,
            "text": "Abstract—Comprehensive IT support teams in large scaleorganizations require more man power for handling engagementand requests of employees from different channels on a 24×7basis. Automated email technical queries help desk is proposedto have instant real-time quick solutions and email categorisation.Email topic modelling with various machine learning, deep-learning approaches are compared with different features for ascalable, generalised solution along with sure-shot static rules.Email’s title, body, attachment, OCR text, and some featureengineered custom features are given as input elements. XGBoostcascaded hierarchical models, Bi-LSTM model with word embed-dings perform well showing 77.3 overall accuracy For the realworld corporate email data set. By introducing the thresholdingtechniques, the overall automation system architecture provides85.6 percentage of accuracy for real world corporate emails.Combination of quick ﬁxes, static rules, ML categorization as alow cost inference solution reduces 81 percentage of the humaneffort in the process of automation and real time implementation.",
            "refs": []
        },
        {
            "index": 1,
            "text": "In an organization, the Information Technology (IT) supporthelp desk operation is an important unit which handles theIT services of a business. Many large scale organizationswould have a comprehensive IT supportteam to handleengagement and requests with employees on a 24×7 basis.As any routinized tasks, most processes of the support helpdesk unit are considered repetitive in nature . Some mayoccur on a daily basis and others may occur more frequently.Many support engineers and agent would spend time on theserepetitive task such as entering information to an application,",
            "refs": [
                1
            ]
        },
        {
            "index": 2,
            "text": "The industry has now come realize that many repetitivebusiness processes and tasks can be automated by usingRobotic Process Automation (RPA) bots or robotic processesautomotive software bots . The idea is to take the repet-itive workload and hand it over to the RPA bots so thatthe employees could focus on more value adding tasks anddecision making to the organization. The RPA bot would alsohelp to reduce the human errors and make processes moreefﬁcient, which would ﬁnally intent results in cost saving andproductivity increase.",
            "refs": []
        },
        {
            "index": 3,
            "text": "Our proposed automated approach is not only focusedon automating repetitive tasks but also looking at historicaldata, enabling IT support desk process to identify unforeseeninsights and patterns. Analyzing the data from various sourcessuch as email communications, service request informationgenerated from support ticketing applications and even con-versational data from chats has helped us to identify the typeof Service Requests (SR) raised and their respective solutions,as well as ﬁxes done by the support agents. This approachhas helped us create a classiﬁcation model to identify theissue types and provide quick ﬁxes and resolutions from thecollected data.",
            "refs": []
        },
        {
            "index": 4,
            "text": "WrÃ ˇCblewska has conducted a project on the topic ofRPA of unstructured data which was focused on building anArtiﬁcial Intelligence (AI) system dedicated to tasks regardingthe processing of formal documents used in different kindsof business procedures . His approach was introduced toautomate the debt collecting process. Possible applications ofMachine Learning (ML) methods to improve the efﬁcacy of",
            "refs": [
                3
            ]
        },
        {
            "index": 5,
            "text": "these processes were described. In the case study done byAguirre, it was concluded that companies should consider RPAto be more suitable for high volume standardized tasks thatare rule-driven, with no requirement for subjective judgement,creativity or interpretation skills . Back ofﬁce business pro-cesses such as accounts payable, accounts receivable, billing,travel and expenses, ﬁxed assets and human resource admin-istration are good candidates for RPA.",
            "refs": []
        },
        {
            "index": 6,
            "text": "Extreme multi-class and multi-label text classiﬁcation prob-lems are solved by the methodology named Hierarchical LabelSet Expansion (HLSE) . This paper presents the deepLearning architecture devoted to text classiﬁcation, in whichthe data labels are regularized, the hierarchical label set isdeﬁned and different word embeddings are used .The traditional model performed better than the the deeplearning models for 8,841 emails collected over 3 years, be-cause this particular classiﬁcation task carried out by Haoranmay not require the ordered sequence representation of tokensthat deep learning models provide . This paper claims thata bagged voting model surpasses the performance of anyindividual models.",
            "refs": [
                6
            ]
        },
        {
            "index": 7,
            "text": "In their survey, Kamran and other researchers analyzed textfeature extractions , dimentionality reduction methods,existing algorithms and techniques, evaluation methods andlimitations  and advantages based on applications. Parameshet al and Seongwook et al compare the different classiﬁcationalgorithms such as multinomial naive bayes logistic regression,K-Nearest neighbour and Support Vector Machines (SVM) onreal-world IT infrastructure ticket classiﬁer system data, usingdifferent evaluation metrics in their research . Theyclaimed that SVM to have performed well on all the data sam-ples. Random forest (RF) or naive bayes (NB) performed bestin terms of correctly uncovering human intuitions. Hartmannet al and his team present in their study that RF exhibits highperformance in sentiment classiﬁcation research done on 41social media data sets covering major social media platforms,where the SVM never outperforms the RF . Cognitive RPAis efﬁciently undertaken as a low cost solution with MicrosoftAzure Language Understanding Intelligent Service (LUIS) and Azure machine learning studio.",
            "refs": [
                9,
                10,
                11,
                12
            ]
        },
        {
            "index": 8,
            "text": "Section III of this paper elaborates the process of automa-tion. The section IV explains about the email classiﬁcationapproach, and the section V illustrates the results and theirrespective analysis. Finally, section VI contains the conclusionof the results.",
            "refs": []
        },
        {
            "index": 9,
            "text": "We are proposing a hybrid-process automation, in whichwe are introducing the automation architecture while adopt-ing the manual process methodology. Incoming emails, thatcannot be classiﬁed or understood by the knowledge base ofthe automation system will be sent for manual classiﬁcationsolution.",
            "refs": []
        },
        {
            "index": 10,
            "text": "mails and matching experts with employees who are in needof that expertise. When a technical issue is raised from a baselevel employee who works with applications, it is sent to themiddle level and then to the higher level management of therespective regional branches throughout the hierarchical busi-ness architecture. Once it is approved by the branch manager,the issue email is forwarded to the technical coordinator tocategorize the issue based on the priority level and technicalrequirements. Technical coordinator is responsible for theissues raised from the regional branches all over the world.",
            "refs": []
        },
        {
            "index": 11,
            "text": "Each regional branch is given a unique name such as NewYork, Sydney, London, Beijing and Toronto mentioned asCategory1 (cat1). Category1 is identiﬁed by looking at theemail address of the sender. Each regional branch has differentplant applications that need different experts’ consultation.Plant applications such as SAP, Darwin and infrastructure arementioned as Category2 (cat2). The possible plot of the issueemails such as computer, manufacturing, userID, userunlock,ﬁnancial, planning, purchasing issue generated by employeesworking in various plant applications across various regionsare mentioned as Category3.",
            "refs": []
        },
        {
            "index": 12,
            "text": "Mapping table is created with the plants placed in theregional ofﬁces and the issues created by the plants. Cate-gory1, Category2, Category3 contains 84, 8 and 77 uniquecategories to be classiﬁed. Table I shows some examples foreach categories. Once all three categories are ﬁnalized bythe technical coordinator, email tickets will be created andassigned to the admin-groups. Respective technical peoplein the admin-groups will provide consultancy and solve theissues. Not only one technician can handle issues assignedto many different admin groups allocated to him, but alsoparticular admin category can be handled by many techniciansas a group as well.",
            "refs": []
        },
        {
            "index": 13,
            "text": "If an incoming email is identiﬁed with the matched intent,cat1, cat2, cat3 will be allocated. Tickets will be createdfor admin-groups. The issue will be solved using automatedmessages through a chat bot solution. If the issue is solved,then the ticket will be closed by the quick ﬁxes. If it is toocomplicated for the knowledge of the BOT then it createsticket for adminGroup for the assistance of consultants.",
            "refs": []
        },
        {
            "index": 14,
            "text": "1) Quickﬁxes: Microsoft LUIS is used for instant quickﬁxes to provide solution based on prioritized emails. Fig.2 shows the bot framework LUIS architecture that handlesthe quick ﬁxes. Quick ﬁxes are trained with most occurringsamples that need quick solutions. LUIS is a modelthatartiﬁcial intelligence applications use to predict the intentionof phrases spoke. There are 3 main key phases categorized asdeﬁning phase, training phase and publishing phase. Naturallanguage is extremely ﬂexible with LUIS. Intents are thetype of deﬁned words that are supported by utterances. Anaction the user wants to perform can be deﬁned by an intent.Fig. 3 elaborates the intent matching breakdown mechanism.Entities are identiﬁed form the sentences. Suitable entity willbe selected for generating tickets.",
            "refs": []
        },
        {
            "index": 15,
            "text": "The emails identiﬁed by static rules and keywords are clas-siﬁed with the highest accuracy. The knowledge base of staticrules and keywords are gathered using feature engineering andthe insights from the technical coordinator. Remaining emailsare sent to a complex ensemble machine learning model to beclassiﬁed.",
            "refs": []
        },
        {
            "index": 16,
            "text": "attachments’ availability. Cat1 is assigned according to theknowledge of the database and sender details. According tothe priority, emails are passed through LUIS. Thereafter ifLUIS fails to solve the issue ML model will assign the cat2,cat3, Admin group for ticket creation.",
            "refs": []
        },
        {
            "index": 17,
            "text": "3) Forwarded mail: If incoming mail is a continuation ofprevious email, it is directly handled by LUIS question andanswer self automated support. Then it follows the normalprocedure of categorization. Fig. 5 clearly illustrates the ﬂow.Fig. 6 explains the overall architecture. Static rules arementioned as T-codes. Every categorized mails has to beassigned to respective consultant denoted as assignTo.",
            "refs": []
        },
        {
            "index": 18,
            "text": "Preprocessing is necessary to increase the accuracy of atext classiﬁcation model, because it avoids the classiﬁcationmodel focusing attention on unwanted sentences and intents.Emails are fed into Microsoft-Bot services. It handles theheaders and outputs the processed channel-data in JavaScriptObject Notation (JSON) format. The channel data summarizesthe information such like sender, receiver, body, subject andimportant metadata. Regular expression (regex) can be used forsearching strings by deﬁning a search pattern. Regex ﬁndingsare created to remove unwanted words from the channel dataqueries for further processing of the emails.",
            "refs": []
        },
        {
            "index": 19,
            "text": "depends on the image quality such as blurry images, smalltext size, complex background, shadows and handwritten text.Since most of the image attachments are computer generatedimages and screen shots of error messages, Microsoft-OCRcapabilities ﬁts for the use case.",
            "refs": []
        },
        {
            "index": 20,
            "text": "260000 emails are taken from past history. Extracted prepro-cessed data from Microsoft-Bot and OCR services are saved asComma-separated Values (CSV) ﬁles. It is further processedbefore feeding to machine learning model. Unwanted wordsare removed from the context using nltk library stopwordsand manually collected stopwords. URLs, punctuation marksare removed. Every word is tokenized, lemmatized and nor-malized, i.e. title, body, OCR, from, to, CC, Cat1, Cat2, andCat3.",
            "refs": []
        },
        {
            "index": 21,
            "text": "Since the sender and receiver varies with time becauseof new employees’ arrivals and old employees’ resignations.In order to handle this ﬂuctuating situation, To, CC, Fromcolumns are dropped from the input data. Cat1 is known fromthe email address. Cat2, Cat3 for speciﬁc cat1 is described inthe table1. Cat2 and Cat3 are merged and deﬁned as targetcategory for classiﬁcation. Nearly 180 custom features arecreated based on the plant’s availability and region mapping.It is embedded to understand the availability of plant andthe issue for the given region denoted as Unique-Category.Based on mapping table (extension of table1), custom featuresensures that whether the plant application (cat2) and thetechnical issue (cat3) belongs to the regional plant (cat1).",
            "refs": []
        },
        {
            "index": 22,
            "text": "By the analysis made from the existing samples and fromthe human semantic knowledge of the technical coordinator,it is sensed that not only the title of the email is enough topredict the category, but also the attachment and body play amajor role.",
            "refs": []
        },
        {
            "index": 23,
            "text": "Even though labelled data set was provided, initially un-supervised learning algorithm K-Nearest Neighbor (KNN)clustering was applied to the data set to observe the possibilityof clusters . Since number of unique categories of thetarget ﬁeld (Unique-Cat) is 77,there are many commonwords between categories. It is too confusing and not showingpromising categories and accuracies. Multi class multi labelclassiﬁcation supervised algorithms such as random forest,XGBoost are used as benchmarks.",
            "refs": []
        },
        {
            "index": 24,
            "text": "• Ngrams are a continuous sequence of n items froma given sample of text. From title, body and OCRtext words are selected. Ngrams of 3 nearby wordsare extracted with Term Frequency-Inverse DocumentFrequency (TF-IDF) vectorizing, then features are ﬁlteredusing chi squared the feature scoring method.",
            "refs": []
        },
        {
            "index": 25,
            "text": "• Feature hashing is a method to extract the features fromthe text. It allows to use variable size of feature vectorswith standard learning algorithms. 12000 features arehashed from the text, OCR and title. Then using chi-squared statistical analysis 200 best features that ﬁts withtarget unique-category are selected.",
            "refs": []
        },
        {
            "index": 26,
            "text": "2) Random forest: Random Forest is a bagging Algorithm,an ensemble learning method for classiﬁcation that operatesby constructing a multitude of decision trees at training timeand outputting the class that has highest mean majority voteof the classes .",
            "refs": [
                15
            ]
        },
        {
            "index": 27,
            "text": "3) XGBoost: XGBoost is a decision-tree-based ensembleMachine Learning algorithm that uses a gradient boostingframework. It is used commonly in the classiﬁcation problemsinvolving unstructured data .",
            "refs": [
                6
            ]
        },
        {
            "index": 28,
            "text": "4) Hierarchical Model: Since the number of target labelsare high, achieving the higher accuracy is difﬁcult, whilekeeping all the categories under same feature selection method.Some categories performs well with lower TF-IDF vectorizingrange and higher n grams features even though they showedlower accuracy in the overall single model. Therefore, hi-erarchical machine learning models are built to classify 31categories in the ﬁrst classiﬁcation model and remaining cat-egories are named as low-accu and predicted as one category.",
            "refs": []
        },
        {
            "index": 29,
            "text": "In the next model, predicted low-accu categories are againclassiﬁed into 47 categories. Comparatively this hierarchicalmodel works well since various feature selection methods areused for various categories .",
            "refs": [
                6
            ]
        },
        {
            "index": 30,
            "text": "1) LSTM: Long short term memory is an artiﬁcial neuralnetwork architecture which outperforms most of the machinelearning algorithms. In the deep learning approach, featureselection is done in neurons weight matrix by itself. Bidirec-tional long short term memory (LSTM) is used with gloveword embedding to predict the categories .",
            "refs": [
                16
            ]
        },
        {
            "index": 31,
            "text": "2) BERT: Even though Bert is the state of the art model, forthe considered data set it hasn’t shown-up with the maximumbreach of accuracy for the expected automation . Whenwe consider the commercial model for the inference, having adedicated Kubernetes cluster with high performance computeris costly. So complex models with high computation powerare not considered as abetter solution.",
            "refs": [
                17
            ]
        },
        {
            "index": 32,
            "text": "An accuracy of 77.3 percentage was obtained without anythresholding techniques for 73 classes multiclasss multi labelclassiﬁcation problem. With threshold adjustments for eachcategories, it was increased to 85.6 percentage. Increasingthreshold values results in reducing the number of mailsclassiﬁed by ML-model. It is necessary to handle limitednumber of high conﬁdent emails by the ML-model due toensure the promising accuracy levels. Feature Engineering forcustom feature selection and, Hierarchical cascade modellingincreases the accuracy of the XGBoost machine learningmodel to reach accuracy of the LSTM models. By cascadingmodel1 (mod1) with 83.2 accuracy for 31 classes and model2(mod2) with 71.1 accuracy for 47 low-accuracy classes, overallhierarchical model exhibited 76.5 accuracy. All the accuracyterms refers F-score. Selected keywords were used as staticrules accurate classiﬁcation. Since accuracy is considerablysatisfactory for the automation process, the system was de-ployed. The incorrectly classiﬁed mails are handled manuallyafter the proper notiﬁcation by the technical consultant.",
            "refs": []
        },
        {
            "index": 33,
            "text": "In order to classify only higher conﬁdent emails, the thresh-olds for each and every 73 categories are deﬁned. For anincoming email, the probability of assigning each categorywill be calculated. Best category will be selected based on themaximum probability out of those 73 probabilities. By lookingat overall F-score, thresholding decisions are made. For thelow accuracy categories (accuracy less than 75 percentage)higher threshold level is set. For middle accuracy categories(accuracy less than 90 percentage) min probability of correctlyclassiﬁed samples are taken. Higher accuracy categories (accu-racy greater than 90 percentage) are left free with 0 thresholdto classify all the incoming emails. The threshold techniquesas a bottle neck decreases the number of samples classiﬁedby the autonomous process, but it increases the accuracy ofthe classiﬁed samples. The proposed thresholds satisfy theexpected manual workload reduction as well as the accuracypercentage.",
            "refs": []
        },
        {
            "index": 34,
            "text": "In this paper Randomforest, XGBoost, LSTM, BidirectionalLSTM with embeddings are analyzed with different input fea-tures. Complex deep-learning models such like transformersare not used in order to go for low cost inference solution.Train set and test set are divided as 80:20 percentage. Preci-sion, recall, F-score are taken as evaluation metrics.",
            "refs": []
        },
        {
            "index": 35,
            "text": "Automation of quick email replies for technical queriesincrease the overall efﬁciency of day to day processes by 3percentage. Even though replacing the manual Human email-assigner entirely with AI bot is not possible, yet the automationML model handles 61 percentage of incoming emails correctly.It is reducing massive human effort per day. For generalizationpurpose email’s title, body, attachments are considered inincreasing accuracy, while ignoring sender, receiver, carboncopy information. Table II shows the accuracy percentagesfor different models with different feature selection methods.",
            "refs": []
        },
        {
            "index": 36,
            "text": "Fig. 7 Shows emails classiﬁed by the ML, static rules andmanual process represented in daily basis. Incoming emailsper day varies between 30 to 120. It clearly illustrates theeffect of retraining. After 10-April, the percentages of emailsclassiﬁed per day was increased as well as accuracy.",
            "refs": []
        },
        {
            "index": 37,
            "text": "Fig. 8 shows average monthly analysis of incoming mailsafter each retraining. Average Monthly incoming mails arecalculated as 1467 per month by considering a 4 monthsperiod. Initial training was done on august 2018 with 170,000samples, model was able to classify nearly 50 percentageof incoming emails. After the second retraining on january2019 with 200,000 sample, model classiﬁed 58 percentageof incoming mails per month. Third retraining was done onApril 2019 with 260000 samples. Results stated that nearly61 percentage of incoming mails were handled by ML model.Nearly 20 percentage of incoming emails were handled bystatic rules. Automation bot was proved to handle 81 percent-age of the total incoming mails per month including ML andstatic rules, leading to efﬁcient human-machine interaction,Instant problem solving and fast process.",
            "refs": []
        },
        {
            "index": 38,
            "text": " A. Wróblewska, T. Stanisławek, B. Prus-Zaj ˛aczkowski, and Ł. Gar-ncarek, “Robotic process automation of unstructured data with machinelearning,” Annals of Computer Science and Information Systems, vol. 16,pp. 9–16, 2018.",
            "refs": [
                3
            ]
        },
        {
            "index": 39,
            "text": " S. Aguirre and A. Rodriguez, “Automation of a business processusing robotic process automation (rpa): A case study,” in Workshop onEngineering Applications. Springer, 2017, pp. 65–71.",
            "refs": []
        },
        {
            "index": 40,
            "text": " F. Gargiulo, S. Silvestri, M. Ciampi, and G. De Pietro, “Deep neuralnetwork for hierarchical extreme multi-label text classiﬁcation,” AppliedSoft Computing, vol. 79, pp. 125–138, 2019.",
            "refs": []
        },
        {
            "index": 41,
            "text": " R. A. Stein, P. A. Jaques, and J. F. Valiati, “An analysis of hierarchicaltext classiﬁcation using word embeddings,” Information Sciences, vol.471, pp. 216–232, 2019.",
            "refs": [
                6
            ]
        },
        {
            "index": 42,
            "text": " K. Kowsari, K. Jafari Meimandi, M. Heidarysafa, S. Mendu, L. Barnes,and D. Brown, “Text classiﬁcation algorithms: A survey,” Information,vol. 10, no. 4, p. 150, 2019.",
            "refs": []
        },
        {
            "index": 43,
            "text": " H. Zhang, J. Rangrej, S. Rais, M. Hillmer, F. Rudzicz, and K. Malikov,“Categorizing emails using machine learning with textual features,” inCanadian Conference on Artiﬁcial Intelligence.Springer, 2019, pp.3–15.",
            "refs": []
        },
        {
            "index": 44,
            "text": " A. Masood and A. Hashmi, “Cognitive robotics process automation:Automate this!” in Cognitive Computing Recipes. Springer, 2019, pp.225–287.",
            "refs": [
                9
            ]
        },
        {
            "index": 45,
            "text": " C. Huang, J. Zhu, Y. Liang, M. Yang, G. P. C. Fung, and J. Luo,“An efﬁcient automatic multiple objectives optimization feature selectionstrategy for internet text classiﬁcation,” International Journal of MachineLearning and Cybernetics, vol. 10, no. 5, pp. 1151–1163, 2019.",
            "refs": [
                10
            ]
        },
        {
            "index": 46,
            "text": " S. Youn and D. McLeod, “A comparative study for email classiﬁca-tion,” in Advances and innovations in systems, computing sciences andsoftware engineering. Springer, 2007, pp. 387–391.",
            "refs": [
                11
            ]
        },
        {
            "index": 47,
            "text": " S. Paramesh and K. Shreedhara, “Automated it service desk systemsusing machine learning techniques,” in Data Analytics and Learning.Springer, 2019, pp. 331–346.",
            "refs": [
                12
            ]
        },
        {
            "index": 48,
            "text": " J. Hartmann, J. Huppertz, C. Schamp, and M. Heitmann, “Comparing au-tomated text classiﬁcation methods,” International Journal of Researchin Marketing, vol. 36, no. 1, pp. 20–38, 2019.",
            "refs": []
        },
        {
            "index": 49,
            "text": " Z. Yong, L. Youwen, and X. Shixiong, “An improved knn text classi-ﬁcation algorithm based on clustering,” Journal of computers, vol. 4,no. 3, pp. 230–237, 2009.",
            "refs": []
        },
        {
            "index": 50,
            "text": " M. Pal, “Random forest classiﬁer for remote sensing classiﬁcation,”International Journal of Remote Sensing, vol. 26, no. 1, pp. 217–222,2005.",
            "refs": [
                15
            ]
        },
        {
            "index": 51,
            "text": " J. Y. Lee and F. Dernoncourt, “Sequential short-text classiﬁcationwith recurrent and convolutional neural networks,” arXiv preprintarXiv:1603.03827, 2016.",
            "refs": [
                16
            ]
        },
        {
            "index": 52,
            "text": " J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-trainingof deep bidirectional transformers for language understanding,” arXivpreprint arXiv:1810.04805, 2018.",
            "refs": [
                17
            ]
        },
        {
            "index": 53,
            "text": "Quick ﬁxes from Microsoft LUIS Bot framework pro-vides instant solutions for the raised email queries. Inputtext features of emails such as title, body, attachment OCRtext and the feature engineered custom features all togetheroutperform for the considered real word email data set. Sure-shot Static rules and hierarchical machine learning modelwith statistically calculated threshold enhances the accuracy ofthe overall system to an acceptable percentage. BidirectionalLSTM with word embedding techniques are implementedﬁnally with thresholding techniques. Less complex Machinelearning models lead to low cost virtual machine solutionsfor serving. Robotic Process Automation Architecture reduceshuman effort of email support desk by 81 percentage whilehaving a reasonable accuracy of 85.6 percentage.",
            "refs": []
        },
        {
            "index": 54,
            "text": " S. Park, A. X. Zhang, L. S. Murray, and D. R. Karger, “Opportunitiesfor automating email processing: A need-ﬁnding study,” in Proceedingsof the 2019 CHI Conference on Human Factors in Computing Systems.ACM, 2019, p. 374.",
            "refs": [
                1
            ]
        },
        {
            "index": 55,
            "text": " F. Al-Hawari and H. Barham, “A machine learning based help desksystem for it service management,” Journal of King Saud University-Computer and Information Sciences, 2019.",
            "refs": []
        }
    ],
    "4": [
        {
            "index": 0,
            "text": "Speaker diarization is the process of partitioning an input audiostream into homogeneous segments according to the speaker iden-tity. It answers the question “who spoke when” in a multi-speakerenvironment. It has a wide variety of applications including multi-media information retrieval, speaker turn analysis, and audio pro-cessing. In particular, the speaker boundaries produced by diariza-tion systems have the potential to signiﬁcantly improve acousticspeech recognition (ASR) accuracy.",
            "refs": []
        },
        {
            "index": 1,
            "text": "A typical speaker diarization system usually consists of fourcomponents: (1) Speech segmentation, where the input audio is seg-mented into short sections that are assumed to have a single speaker,and the non-speech sections are ﬁltered out; (2) Audio embeddingextraction, where speciﬁc features such as MFCCs , speaker fac-tors , or i-vectors  are extracted from the segmented sec-tions; (3) Clustering, where the number of speakers is determined,and the extracted audio embeddings are clustered into these speak-ers; and optionally (4) Resegmentation , where the clustering re-sults are further reﬁned to produce the ﬁnal diarization results.",
            "refs": [
                1,
                2,
                3,
                4,
                5
            ]
        },
        {
            "index": 2,
            "text": "In recent years, neural network based audio embeddings (d-vectors) have seen wide-spread use in speaker veriﬁcation applica-tions , often signiﬁcantly outperforming previouslystate-of-the-art techniques based on i-vectors. However, most ofthese applications belong to text-dependent speaker veriﬁcation,where the speaker embeddings are extracted from speciﬁc detected",
            "refs": [
                7,
                8,
                9,
                10,
                11
            ]
        },
        {
            "index": 3,
            "text": "keywords .independent embeddings which work on arbitrary speech.",
            "refs": [
                12,
                13
            ]
        },
        {
            "index": 4,
            "text": "In this paper, we explore a text-independent d-vector based ap-proach to speaker diarization. We leverage the work of  to trainan LSTM-based text-independent speaker veriﬁcation model, thencombine this model with recent work in non-parametric spectralclustering algorithm to obtain a state-of-the-art speaker diarizationsystem.",
            "refs": [
                11
            ]
        },
        {
            "index": 5,
            "text": "While several authors have had explored using neural networkembeddings for diarization tasks, their work has largely focused onusing feed-forward DNNs to directly perform diarization. For exam-ple,  uses DNN embeddings trained on PLDA-inspired loss. Incontrast, our work uses RNNs (speciﬁcally LSTMs ), which bet-ter capture the sequential nature of audio signals, and our generalizedend-to-end training architecture directly simulates the enroll-verifyrun-time logic.",
            "refs": [
                14,
                15
            ]
        },
        {
            "index": 6,
            "text": "There have been several attempts to apply spectral clustering to the speaker diarization problem . However, to theauthors’ knowledge, our work is the ﬁrst to combine LSTM-basedd-vector embeddings with spectral clustering. Furthermore, as partof our spectral clustering algorithm, we present a novel sequenceof afﬁnity matrix reﬁnement steps which act to de-noise the afﬁnitymatrix, and are crucial to the success of our system.",
            "refs": [
                16,
                17,
                3
            ]
        },
        {
            "index": 7,
            "text": "The remainder of this paper is organized as follows: In Sec. 2,we describe how the LSTM-based text-independent speaker veriﬁ-cation model trained with the framework in  can be adapted tofeaturize raw audio data and prepare it for clustering. In Sec. 3, wedescribe four different clustering algorithms and discuss the pros andcons of each in the context of speaker diarization, culminating witha modiﬁed spectral clustering algorithm. Experimental results anddiscussions are presented in Sec. 4, and conclusions are in Sec. 5.",
            "refs": [
                11
            ]
        },
        {
            "index": 8,
            "text": "Wan et al. recently introduced an LSTM-based  speaker embed-ding network for both text-dependent and text-independent speakerveriﬁcation . Their model is trained on ﬁxed-length segmentsextracted from a large corpus of arbitrary speech. They showed thatthe d-vector embeddings produced by such networks usually signiﬁ-cantly outperform i-vectors in an enrollment-veriﬁcation 2-stage ap-plication. We now describe how this model can be modiﬁed for pur-poses of speaker diarization.",
            "refs": [
                15,
                11
            ]
        },
        {
            "index": 9,
            "text": "The ﬂowchart of our diarization system is provided in Fig. 1. Inthis system, audio signals are ﬁrst transformed into frames of width25ms and step 10ms, and log-mel-ﬁlterbank energies of dimension40 are extracted from each frame as the network input. We buildsliding windows of a ﬁxed length on these frames, and run the LSTMnetwork on each window. The last-frame output of the LSTM is thenused as the d-vector representation of this sliding window.",
            "refs": []
        },
        {
            "index": 10,
            "text": "embedding is available, we compute its similarities to centroids ofall existing clusters. If they are all smaller than the threshold, thencreate a new cluster containing only this embedding; otherwise, addthis embedding to the most similar cluster and update the centroid.",
            "refs": []
        },
        {
            "index": 11,
            "text": "3.2. Links online clusteringLinks is an online clustering method we developed to improve uponthe naive approach. It estimates cluster probability distributions andmodels their substructure based on the embedding vectors receivedso far. The technical details are described in a separate paper .",
            "refs": []
        },
        {
            "index": 12,
            "text": "3.3. K-Means ofﬂine clusteringLike in many diarization systems , we integrated the K-Means clustering algorithm with our system. Speciﬁcally, we use K-Means++ for initialization . To determine the number of speak-",
            "refs": [
                19,
                3
            ]
        },
        {
            "index": 13,
            "text": "1. Construct the afﬁnity matrix A, where Aij is the cosine sim-ilarity between ith and jth segment embedding when i (cid:54)= j,and the diaginal elements are set to the maximal value in eachrow: Aii = maxj(cid:54)=i Aij.",
            "refs": []
        },
        {
            "index": 14,
            "text": "smaller than this row’s p-percentile to 0; 2(c) Symmetrization: Yij = max(Xij, Xji);(d) Diffusion: Y = XX T ;(e) Row-wise Max Normalization: Yij = Xij/ maxk Xik.These reﬁnements act to both smooth and denoise the datain the similarity space as shown in Fig. 2, and are crucialto the success of the algorithm. The reﬁnements are basedon the temporal locality of speech data — contiguous speechsegments should have similar embeddings, and hence similarvalues in the afﬁnity matrix.We now provide the intuition behind each of these opera-tions: The Gaussian blur acts to smooth the data, and re-duce the effect of outliers. Row-wise thresholding serves tozero-out afﬁnities between embeddings belonging to two dif-ferent speakers. Symmetrization restores matrix symmetrywhich is crucial to the spectral clustering algorithm. The dif-fusion steps draws inspiration from the Diffusion Maps algo-rithm , and serves to sharpen the image resulting in clearboundaries between sections of the afﬁnity matrix belongingto distinct speakers. Finally, the row-wise max normalizationserves to rescale the spectrum of the matrix to ensure undesir-able scale effects do not occur during the subsequent spectralclustering step.",
            "refs": []
        },
        {
            "index": 15,
            "text": "We use a Voice Activity Detector (VAD) to determine speechsegments from the audio, which are further divided into smaller non-overlapping segments using a maximal segment-length limit (e.g.400ms in our experiments), which determines the temporal resolu-tion of the diarization results. For each segment, the correspondingd-vectors are ﬁrst L2 normalized, then averaged to form an embed-ding of the segment.",
            "refs": []
        },
        {
            "index": 16,
            "text": "The above process serves to reduce arbitrary length audio inputinto a sequence of ﬁxed-length embeddings. We can now apply aclustering algorithm to these embeddings in order to determine thenumber of unique speakers, and assign each part of the audio to aspeciﬁc speaker.",
            "refs": []
        },
        {
            "index": 17,
            "text": "In this section, we introduce the four clustering algorithms that weintegrated into our diarization system. We place particular focus onthe spectral ofﬂine clustering algorithm, which signiﬁcantly outper-formed the alternative approaches across experiments.",
            "refs": []
        },
        {
            "index": 18,
            "text": "Ofﬂine clustering algorithms typically outperform Online clusteringalgorithms due to the additional contextual information available inthe ofﬂine setting. Furthermore, a ﬁnal resegmentation step can onlybe applied in the ofﬂine setting. Nonetheless, the choice between on-line and ofﬂine depends primarily on the nature of the application —where the system is intended to be deployed. For example, latency-sensitive applications such as live video analysis typically restrict thesystem to online clustering algorithms.",
            "refs": []
        },
        {
            "index": 19,
            "text": "This is a prototypical online clustering algorithm. We apply a thresh-old on the similarities between embeddings of segments. To be con-sistent with the generalized end-to-end training architecture , co-sine similarity is used as our similarity metric.",
            "refs": [
                11
            ]
        },
        {
            "index": 20,
            "text": "(ii) Cluster Imbalance: In speech data, it is often the case thatone speaker will speak often, while other speakers will speakrarely. In this setting, K-Means may incorrectly split largeclusters into several smaller clusters.",
            "refs": []
        },
        {
            "index": 21,
            "text": "(iii) Hierarchical Structure: Speakers fall into various groupsaccording to gender, age, accent, etc. This structure is prob-lematic since the difference between a male and a femalespeaker is much larger than the difference between two fe-male speakers. This makes it difﬁcult for K-Means to dis-tinguish between clusters corresponding to groups, and clus-ters corresponding to distinct speakers. In practice, this oftencauses K-Means to incorrectly cluster all embeddings corre-sponding to male speakers into one cluster, and all embed-dings corresponding to female speakers into another.",
            "refs": []
        },
        {
            "index": 22,
            "text": "The problems caused by these properties are not limited to K-Means clustering, but are endemic to most parametric clustering al-gorithms. Fortunately, these problems can be mitigated by employ-ing a non-parametric connection-based clustering algorithm such asspectral clustering.",
            "refs": []
        },
        {
            "index": 23,
            "text": "The i-vector model is trained using 13 PLP coefﬁcients withdelta and delta-delta coefﬁcients. The GMM-UBM includes 512Gaussians, and the total variability matrix includes 100 eigen-vectors. The ﬁnal i-vectors are reduced to 50-dimensional usingLDA.",
            "refs": []
        },
        {
            "index": 24,
            "text": "The d-vector model is a 3-layer LSTM network with a ﬁnal lin-ear layer. Each LSTM layer has 768 nodes, with projection  of256 nodes.",
            "refs": [
                23
            ]
        },
        {
            "index": 25,
            "text": "Our Voice Activity Detection (VAD) model is a very smallGMM model using the same PLP features as i-vector.It onlyhas two full covariance Gaussians: one for speech, and one fornon-speech. We found this simple VAD generalizes better acrossdomains (from queries to telephone) for diarization than CLDNN VAD models.",
            "refs": []
        },
        {
            "index": 26,
            "text": "We report Diarization Error Rates (DER) on three standard publicdatasets: (1) CALLHOME American English  (LDC97S42 +LDC97T14); (2) 2003 NIST Rich Transcription (LDC2007S10), theEnglish conversational telephone speech (CTS) part; and (3) 2000NIST Speaker Recognition Evaluation (LDC2001S97), Disk-8.",
            "refs": [
                25
            ]
        },
        {
            "index": 27,
            "text": "The third dataset is used by most diarization papers, and is usu-ally directly referred to as “CALLHOME” in literature. It contains500 utterances distributed across six languages: Arabic, English,German, Japanese, Mandarin, and Spanish.",
            "refs": []
        },
        {
            "index": 28,
            "text": "Our diarization evaluation system is based on the pyannote.metricslibrary .",
            "refs": []
        },
        {
            "index": 29,
            "text": "The CALLHOME American English dataset has a default 20-vs-20 utterances division for Dev-vs-Eval. For NIST RT-03 CTS,we randomly divide the 72 utterances into 14-vs-58 Dev and Evalsets. For each diarization system, we tune the parameters such asVoice Activity Detector (VAD) threshold, LSTM window size/step(Fig. 1), and clustering parameters on the Dev set, and report theDER on the Eval set.",
            "refs": []
        },
        {
            "index": 30,
            "text": "For NIST RT-03 CTS, we only report DERs based on those pro-vided un-partitioned evaluation map (UEM) ﬁles. For the other twodatasets, as is the standard convention in literature ,we tolerate errors less than 250ms in locating segment boundaries.As is typical, for each audio ﬁle, multiple channels are mergedinto a single channel , and we do not process the parts thatare before the ﬁrst annotation or after the last annotation. Addi-tionally, as is standard in literature, we exclude overlapped speech(multiple speakers speaking at the same time) from our evaluation.For ofﬂine clustering algorithms, we constrain the system to produceat least 2 speakers.",
            "refs": [
                2,
                3,
                4,
                14
            ]
        },
        {
            "index": 31,
            "text": "Table 2: DER (%) on NIST SRE 2000 CALLHOME. Since wedidn’t do resegmentation, we report others’ work by listing both with& without Variational Bayesian (VB) resegmentation . Note thatunlike others’ work, our model is trained with out-of-domain data(English voice search vs. multilingual telephone speech).",
            "refs": []
        },
        {
            "index": 32,
            "text": "d-vector + spectralCastaldo et al. Shum et al. ",
            "refs": [
                2,
                3
            ]
        },
        {
            "index": 33,
            "text": "Senoussaoui et al. Sell et al.  (+VB)",
            "refs": [
                4
            ]
        },
        {
            "index": 34,
            "text": "4.4. ResultsOur experimental results are shown in Table 1, 2 and 3. We reportthe total DER together with its three components: False Alarm (FA),Miss, and Confusion. FA and Miss are mostly from Voice ActivityDetection errors, and partly from the aggregation from frame-leveli-vectors or window-level d-vectors to segments. The FA and Missdifferences between i-vector and d-vector are due to their differentwindow sizes/steps and aggregation logics.",
            "refs": []
        },
        {
            "index": 35,
            "text": "In Table 1, we can see that d-vector based diarization systemssigniﬁcantly outperform i-vector based systems. For d-vector sys-tems, the optimal sliding window size and step are 240ms and120ms, respectively.",
            "refs": []
        },
        {
            "index": 36,
            "text": "We also observe that as expected, ofﬂine diarization producessigniﬁcantly better results than online diarization. Speciﬁcally, on-line diarization predicts the incorrect number of speakers much morefrequently than ofﬂine diarization. This problem could potentiallybe mitigated by the addition of a “burn-in” stage before entering theonline mode.",
            "refs": []
        },
        {
            "index": 37,
            "text": "In Table 2, we compare our d-vector + spectral clustering systemwith others’ work on the same dataset. Though our LSTM modelis completely trained on out-of-domain and English-only data, wecan still achieve state-of-the-art performance on this multilingualdataset. The performance could potentially be further improved byusing in-domain training data and adding a ﬁnal resegmentation step.Additionally, in Table 3, we followed the same practice in to evaluate our system on a subset of 109 utterances from CALL-HOME American English that have 2 speakers (called CH-109 in). Number of speakers is ﬁxed to 2 for this evaluation.",
            "refs": []
        },
        {
            "index": 38,
            "text": "tunately common problem in the diarization community. This is dueprimarily to the large number of moving parts required for a func-tional diarization pipeline. For example, different teams use differ-ent Voice Activity Detection marks (not publicly available), differenttraining datasets, and different Dev sets for parameter tuning.",
            "refs": []
        },
        {
            "index": 39,
            "text": "The evaluation protocols and software also differ from paper topaper. Most teams exclude FA and Miss from their evaluations, anddirectly refer to Confusion as their DER. However, we observed thata poor VAD with high Miss usually ﬁlters out the difﬁcult parts in thespeech, and makes the clustering problem much easier. Some paperslike  use the non-standard Speaker Clustering Errors in framepercentage as their metric, and also exclude FA and Miss from thiserror. Additionally, it’s unclear how overlapped speech is handled insome papers.",
            "refs": []
        },
        {
            "index": 40,
            "text": "In this paper, we built on the success of d-vector based speaker veriﬁ-cation systems to develop a new d-vector based approach to speakerdiarization. Speciﬁcally, we combined LSTM-based d-vector audioembeddings with recent work in non-parametric clustering to obtaina state-of-the-art speaker diarization system. We conducted exper-iments on four clustering algorithms combined with both i-vectorsand d-vectors, and reported the performance on three standard pub-lic datasets: CALLHOME American English, NIST RT-03 EnglishCTS, and NIST SRE 2000. In general, we observed that d-vectorbased systems achieve signiﬁcantly lower DER than i-vector basedsystems.",
            "refs": []
        },
        {
            "index": 41,
            "text": "We would like to thank Dr. Herv´e Bredin for the continuous supportwith the pyannote.metrics library. We would like to thank Dr.Gregory Sell and Prof. Pietro Laface for helping us understand theevaluation datasets. We would like to thank Yash Sheth and RichardRose for the helpful discussions.",
            "refs": []
        },
        {
            "index": 42,
            "text": " Patrick Kenny, Douglas Reynolds, and Fabio Castaldo, “Di-arization of telephone conversations using factor analysis,”IEEE Journal of Selected Topics in Signal Processing, vol. 4,no. 6, pp. 1059–1070, 2010.",
            "refs": [
                1
            ]
        },
        {
            "index": 43,
            "text": " Fabio Castaldo, Daniele Colibro, Emanuele Dalmasso, PietroLaface, and Claudio Vair,“Stream-based speaker segmen-tation using speaker factors and eigenvoices,” in Acoustics,Speech and Signal Processing, 2008. ICASSP 2008. IEEE In-ternational Conference on. IEEE, 2008, pp. 4133–4136.",
            "refs": [
                2
            ]
        },
        {
            "index": 44,
            "text": " Stephen H Shum, Najim Dehak, R´eda Dehak, and James RGlass, “Unsupervised methods for speaker diarization: An in-tegrated and iterative approach,” IEEE Transactions on Audio,Speech, and Language Processing, vol. 21, no. 10, pp. 2015–2028, 2013.",
            "refs": [
                3
            ]
        },
        {
            "index": 45,
            "text": " Mohammed Senoussaoui, Patrick Kenny, Themos Stafylakis,and Pierre Dumouchel,“A study of the cosine distance-based mean shift for telephone speech diarization,” IEEE/ACMTransactions on Audio, Speech and Language Processing(TASLP), vol. 22, no. 1, pp. 217–227, 2014.",
            "refs": [
                4
            ]
        },
        {
            "index": 46,
            "text": " Gregory Sell and Daniel Garcia-Romero, “Speaker diariza-tion with plda i-vector scoring and unsupervised calibration,”in Spoken Language Technology Workshop (SLT), 2014 IEEE.IEEE, 2014, pp. 413–417.",
            "refs": [
                5
            ]
        },
        {
            "index": 47,
            "text": " Gregory Sell and Daniel Garcia-Romero,",
            "refs": []
        },
        {
            "index": 48,
            "text": " Ehsan Variani, Xin Lei, Erik McDermott, Ignacio LopezMoreno, and Javier Gonzalez-Dominguez, “Deep neural net-works for small footprint text-dependent speaker veriﬁcation,”in Acoustics, Speech and Signal Processing (ICASSP), 2014IEEE International Conference on. IEEE, 2014, pp. 4052–4056.",
            "refs": [
                7
            ]
        },
        {
            "index": 49,
            "text": " Yu-hsin Chen, Ignacio Lopez-Moreno, Tara N Sainath, Mirk´oVisontai, Raziel Alvarez, and Carolina Parada,“Locally-connected and convolutional neural networks for small foot-print speaker recognition,” in Sixteenth Annual Conference ofthe International Speech Communication Association, 2015.",
            "refs": [
                8
            ]
        },
        {
            "index": 50,
            "text": " Georg Heigold, Ignacio Moreno, Samy Bengio, and NoamShazeer,“End-to-end text-dependent speaker veriﬁcation,”in Acoustics, Speech and Signal Processing (ICASSP), 2016IEEE International Conference on. IEEE, 2016, pp. 5115–5119.",
            "refs": [
                9
            ]
        },
        {
            "index": 51,
            "text": " F A Rezaur Rahman Chowdhury, Quan Wang, Li Wan,“Attention-based modelsarXiv preprint",
            "refs": [
                10
            ]
        },
        {
            "index": 52,
            "text": " Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno,“Generalized end-to-end loss for speaker veriﬁcation,” arXivpreprint arXiv:1710.10467, 2017.",
            "refs": [
                11
            ]
        },
        {
            "index": 53,
            "text": " Guoguo Chen, Carolina Parada, and Georg Heigold, “Small-footprint keyword spotting using deep neural networks,” inAcoustics, Speech and Signal Processing (ICASSP), 2014IEEE International Conference on. IEEE, 2014, pp. 4087–4091.",
            "refs": [
                12
            ]
        },
        {
            "index": 54,
            "text": " Rohit Prabhavalkar, Raziel Alvarez, Carolina Parada, PreetumNakkiran, and Tara N Sainath, “Automatic gain control andmulti-style training for robust small-footprint keyword spottingwith deep neural networks,” in Acoustics, Speech and SignalProcessing (ICASSP), 2015 IEEE International Conference on.IEEE, 2015, pp. 4704–4708.",
            "refs": [
                13
            ]
        },
        {
            "index": 55,
            "text": " Daniel Garcia-Romero, David Snyder, Gregory Sell, DanielPovey, and Alan McCree, “Speaker diarization using deep neu-ral network embeddings,” in 2017 IEEE International Confer-ence on Acoustics, Speech and Signal Processing (ICASSP).IEEE, 2017, pp. 4930–4934.",
            "refs": [
                14
            ]
        },
        {
            "index": 56,
            "text": " Sepp Hochreiter and J¨urgen Schmidhuber, “Long short-termmemory,” Neural computation, vol. 9, no. 8, pp. 1735–1780,1997.",
            "refs": [
                15
            ]
        },
        {
            "index": 57,
            "text": " Ulrike Von Luxburg, “A tutorial on spectral clustering,” Statis-",
            "refs": [
                16
            ]
        },
        {
            "index": 58,
            "text": " Huazhong Ning, Ming Liu, Hao Tang, and Thomas S Huang,“A spectral clustering approach to speaker diarization.,” in IN-TERSPEECH, 2006.",
            "refs": [
                17
            ]
        },
        {
            "index": 59,
            "text": " Philip Andrew Mansﬁeld, Quan Wang, Carlton Downey,“Links: A high-arXiv preprint",
            "refs": []
        },
        {
            "index": 60,
            "text": " Oshry Ben-Harush, Ortal Ben-Harush, Itshak Lapidot, andHugo Guterman, “Initialization of iterative-based speaker di-arization systems for telephone conversations,” IEEE Transac-tions on Audio, Speech, and Language Processing, vol. 20, no.2, pp. 414–425, 2012.",
            "refs": [
                19
            ]
        },
        {
            "index": 61,
            "text": " Dimitrios Dimitriadis and Petr Fousek, “Developing on-line",
            "refs": []
        },
        {
            "index": 62,
            "text": " David Arthur and Sergei Vassilvitskii, “k-means++: The ad-vantages of careful seeding,” in Proceedings of the eighteenthannual ACM-SIAM symposium on Discrete algorithms. Soci-ety for Industrial and Applied Mathematics, 2007, pp. 1027–1035.",
            "refs": []
        },
        {
            "index": 63,
            "text": " Ronald R Coifman and St´ephane Lafon, “Diffusion maps,”Applied and computational harmonic analysis, vol. 21, no. 1,pp. 5–30, 2006.",
            "refs": []
        },
        {
            "index": 64,
            "text": " Has¸im Sak, Andrew Senior, and Franc¸oise Beaufays, “Longshort-term memory recurrent neural network architectures forlarge scale acoustic modeling,” in Fifteenth Annual Conferenceof the International Speech Communication Association, 2014. Rub´en Zazo Candil, Tara N Sainath, Gabor Simko, and Car-olina Parada, “Feature learning with raw-waveform cldnns forvoice activity detection,” 2016.",
            "refs": [
                23
            ]
        },
        {
            "index": 65,
            "text": " A Canavan, D Graff, and G Zipperlen, “Callhome americanenglish speech ldc97s42,” LDC Catalog. Philadelphia: Lin-guistic Data Consortium, 1997.",
            "refs": [
                25
            ]
        },
        {
            "index": 66,
            "text": " Herv´e Bredin, “pyannote.metrics: a toolkit for repro-ducible evaluation, diagnostic, and error analysis of speakerdiarization systems,” hypothesis, vol. 100, no. 60, pp. 90, 2017. Zbyn˘ek Zaj´ıc, Marek Hr´uz, and Lud˘ek M¨uller, “Speaker di-arization using convolutional neural network for statistics ac-cumulation reﬁnement,” in INTERSPEECH, 2017.",
            "refs": []
        }
    ],
    "5": [
        {
            "index": 0,
            "text": "Abstract— We present a robot navigation system that usesan imitation learning framework to successfully navigate incomplex environments. Our framework takes a pre-built 3Dscan of a real environment and trains an agent from pre-generated expert trajectories to navigate to any position givena panoramic view of the goal and the current visual inputwithout relying on map, compass, odometry, GPS or relativeposition of the target at runtime. Our end-to-end trained agentuses RGB and depth (RGBD) information and can handle largeenvironments (up to 1031m2) across multiple rooms (up to 40)and generalizes to unseen targets. We show that when comparedto several baselines using deep reinforcement learning andRGBD SLAM, our method (1) requires fewer training examplesand less training time, (2) reaches the goal location with higheraccuracy, (3) produces better solutions with shorter paths forlong-range navigation tasks, and (4) generalizes to unseenenvironments given an RGBD map of the environment.",
            "refs": []
        },
        {
            "index": 1,
            "text": "The ability to navigate efﬁciently and accurately withinan environment is fundamental to intelligent behavior andhas been a focus of research in robotics for many years.Traditionally, robotic navigation is solved using model-basedmethods with an explicit focus on position inference andmapping, such as Simultaneous Localization and Mapping(SLAM) . These models use path planning algorithms,such as Probabilistic Roadmaps (PRM)  and RapidlyExploring Random Trees (RRT)  to plan a collision-free path. These methods ignore the rich information fromvisual input and are highly sensitive to robot odometry andnoise in sensor data. For example, a robot navigating througha room may lose track of its position due to the navigationsoftware not properly modeling friction.",
            "refs": [
                1,
                2,
                3,
                4
            ]
        },
        {
            "index": 2,
            "text": "Model-free reinforcement learning (RL) agents have per-formed well on many robotic tasks , leadingresearchers to rely on RL for robotic navigation tasks . Recent work in robotic visual navigationuses reinforcement learning which trains an agent to navigate∗Authors have contributed equally and names are in alphabetical order.1Department of Computer Science, Columbia University, New York,NY, USA. {davidwatkins, allen}@cs.columbia.edu,jingxi.xu@columbia.edu",
            "refs": [
                5,
                6,
                8,
                9,
                11,
                12
            ]
        },
        {
            "index": 3,
            "text": "This work is supported by NSF Grant CMMI 1734557. This researchwas sponsored by the Army Research Laboratory and was accomplishedunder Cooperative Agreement Number W911NF-18-2-0244. The views andconclusions contained in this document are those of the authors and shouldnot be interpreted as representing the ofﬁcial policies, either expressed orimplied, of the Army Research Laboratory or the U.S. Government. TheU.S. Government is authorized to reproduce and distribute reprints forGovernment purposes notwithstanding any copyright notation herein.",
            "refs": []
        },
        {
            "index": 4,
            "text": "Fig. 1: A successful trajectory executed in house17 fromthe Matterport3D dataset. The history buffer and current vieware the state of the pipeline. The panoramic goal is 8 RGBDimages each taken at a 45◦ turn. The top down view is theagent moving through the trajectory with the blue sphere asthe start position and the green sphere as the goal position.Smaller blue spheres are positions that the agent has beento and the orange spheres are the remaining positions. Theimages are taken at the current position of the robotic agent.",
            "refs": []
        },
        {
            "index": 5,
            "text": "to a goal using only the current and goal RGB images .While reinforcement learning has the convenience of beingweakly supervised, it generally suffers from sparse rewardsin navigation, requires a huge number of training episodesto converge, and struggles to generalize to unseen targets.The problem is further exacerbated when the navigationenvironment becomes large and complex (across multiplerooms and scenes with various obstacles), leading to difﬁcultlong-range path solutions.",
            "refs": [
                9
            ]
        },
        {
            "index": 6,
            "text": "New advancements in annotated 3D maps of real worlddata, such as Stanford2D3DS  and Matterport3D ,enable the collection of large amounts of trajectory data.Simulators capable of collecting this data have arisen in thepast few years in the form of MINOS , Gibson ,Habitat , and THOR . These systems enable simulta-",
            "refs": [
                13,
                15,
                16,
                9
            ]
        },
        {
            "index": 7,
            "text": "neous use of real and simulated environments for training,without the need for visual domain adaptation. Gibson features real-world and photo-realistic data generated fromfully scanned 3D homes and buildings that allow for easiercollection of demonstration data for supervised learning.",
            "refs": [
                16
            ]
        },
        {
            "index": 8,
            "text": "Our work focuses on exploring supervised methods (inparticular, imitation learning) to bring better performanceto robotic visual navigation, while taking advantage of thecurrent progress in robotic simulators and datasets to efﬁ-ciently collect training data and handle the sim-to-real gapand domain adaptation. Our contributions are as follows: (1)we provide a navigation pipeline where the agent learns tonavigate to unseen targets using the current RGBD viewand a novel 8-image panoramic goal without using GPS,compass, map, or relative position of goals at runtime, (2) weprovide a framework to efﬁciently generate optimal experttrajectories in the Gibson simulator using a 3D scan of theenvironment of interest, and (3) we provide a methodologyfor discretizing a continuous trajectory into a series of{forward, right, left} commands. Our method outperformsalternative methods in both the quality of the solution pathsand the success rate, with signiﬁcantly fewer training exam-ples and less training time. Videos, dataset and source codecan be found at http://crlab.cs.columbia.edu/learning_your_way/.",
            "refs": []
        },
        {
            "index": 9,
            "text": "Previous work in visual navigation  provides a target-driven reinforcement learning framework for robotic visualnavigation. Our method shares the same objective of navi-gating to the goal position using the goal image, the currentimage and a sequence of history images.  trains anRL agent to navigate in realistic cluttered environments usinga PointGoal (a speciﬁc location for the goal target). Theyassume an idealized GPS which provides the relative goalposition and use this information to train their agents. Both and  claim that their learned policy generalizes acrosstargets and environments.  only evaluates their method onnew targets that are several steps away from the targets thatthe agent is trained on and the scene-speciﬁc layer has tobe retrained for the policy to work in a new environment. relies on an idealized GPS and the speciﬁc location ofthe goal and it generalizes to new environments by learninga bug algorithm like behavior to follow the boundaries.Imagine a scenario where a person is placed into a buildingthey have not seen before with nothing but an image ofthe place they need to get to. It would be unfair for us toexpect this person to navigate to the target location in anyefﬁcient manner. Therefore, a robotic agent would be unableto handle generalizing to new untrained environments usingvision alone and we focus on the ability to generalize to anyunseen target in the environment that the robotic agent hasseen before.",
            "refs": [
                9,
                18
            ]
        },
        {
            "index": 10,
            "text": "Previous work  uses a synthetic 3D maze environmentand the agent is trained on a single goal. Another work trains an agent to navigate using real world Google Map",
            "refs": [
                11,
                19
            ]
        },
        {
            "index": 11,
            "text": "views with the goal coordinate provided. present hierarchical robot navigation methods using rein-forcement learning to learn local and short-range obstacleavoidance tasks and using sampling-based path planningalgorithms as global planners. These methods use 1D lidarsensor data and a dynamic goal position as input.  use value iteration networks  to learn navigationstrategies in simplistic synthetic simulated environments. evaluates different representations for target-driven vi-sual navigation using a semantic target and an off-the-shelfsegmenter.  presents a method with an interactive worldmodel to navigate to a ﬁxed goal in a known environment.None of these works solve the problem of indoor visualnavigation because they either make compromises in sensoryinput (1D lidar, goal position) or have a different environ-mental design (synthetic maps, outdoor data).",
            "refs": [
                12,
                10,
                20,
                21,
                24,
                25
            ]
        },
        {
            "index": 12,
            "text": "b) Supervised learning methods for navigation: Be-cause most work using learning methods for robotic naviga-tion relies on deep RL, supervised methods are less explored. uses Convolutional Neural Networks (CNNs) forrobotic navigation. However, their methods are different fromours in that they use odometry and a goal location as inputinto their networks and their models are trained only forsimple tasks such as collision detection and localization.",
            "refs": []
        },
        {
            "index": 13,
            "text": "c) Datasets and simulators for navigation environ-ments: As the broader area of active and embodied per-ception has received increased interest, new datasets (Stan-ford2D3DS , Matterport3D , SUNCG , Gib-son ) and simulators (MINOS , Gibson , Habi-tat , AI2THOR ) have been created for roboticnavigation. These new environments enable researchers totrain an agent in simulation using real world data and obtaintraining data much faster than would be possible in the realworld alone. They allow agents to be trained at scale usingground truth positioning and fast rendering. The Gibsonsimulator uses PyBullet  to simulate collisions with theenvironment as well as dynamic environment tasks.",
            "refs": [
                13,
                16,
                15,
                29
            ]
        },
        {
            "index": 14,
            "text": "to au-tonomously navigate to a target position, described by a setof panoramic images taken at the goal, without providingany odometry, GPS or relative location of the target butonly RGBD input from the robot’s point of view. We donot require a particular orientation at the target location.The problem is referred to as target-driven visual navi-gation in the literature , where the task objective (i.e.,navigation destination) is speciﬁed as inputs to our model.Traditional learning-based visual navigation methods havelargely focused on learning goal-speciﬁc models that tackleindividual tasks in isolation, where the goal information ishard-coded in the neural network representations, leadingto poor generalization to unseen / unexplored targets. Target-driven approaches learn to navigate to new targets withoutre-training, using a single navigation pipeline.Our navigation pipeline, denoted as Π,",
            "refs": [
                9
            ]
        },
        {
            "index": 15,
            "text": "The left / right action indicates turning the agent in placeleft / right 10 degrees and the forward action moves theagent 0.1m ahead. The unknown transition model Γ of theenvironment updates the state, denoted by si+1 = Γ(si,ai),when an action ai is executed. The objective is that givenany goal g in the map, a maximum number of steps T , anda success threshold ζ , our navigation pipeline Π can generatea sequence of actions {ai},i ∈ , which satisﬁes (1) t < T ,(2) at = done, (3) the ﬁnal location of the robot is within ζmeter of the target location, and (4) the length of the pathshould be as short as possible. Our navigation pipeline isfully automated as it learns to stop at the goal and does notrequire human intervention through the whole process.",
            "refs": []
        },
        {
            "index": 16,
            "text": "The state si is the current RGBD visual observation and ahistory buffer of 4 concatenated past RGBD images, both ofwhich are from the agent’s viewpoint. The goal informationg is a set of 8 panoramic RGBD images. An example of thestate and the goal information is shown in Figure 1.",
            "refs": []
        },
        {
            "index": 17,
            "text": "The autoencoder model generates latent representations(i.e., embeddings) for both the state si and the goal g,denoted A(si) and A(g). The policy model takes two inputs,the embeddings of the current state and the embeddingsof the target, and produces a probability distribution overthree actions, ai ∈ {forward, right, left} ∼ E (A(si),A(g))).It then picks the action with highest probability from thisdistribution. The policy model is responsible for leading theagent towards the goal with as little exploration as possible.The goal checking model is a binary function which takes thesame input as the policy model, and decides if the agent hasreached the target or not, denoted by G(A(si),A(g)) ∈ {1,0},where 1 corresponds to done and 0 corresponds to not done.An overview of the navigation pipeline is shown in Figure 2.",
            "refs": []
        },
        {
            "index": 18,
            "text": "into our neural network models isRGBD images,if we useembeddings of the raw input. Instead of extracting featuresfrom an intermediate layer of a pre-trained classiﬁer such asResNet-50 , we train an autoencoder from imagescaptured from the same environment.",
            "refs": [
                9
            ]
        },
        {
            "index": 19,
            "text": "Similar to RedNet , our autoencoder network is basedon a 6-layer CNN with batch normalization on every layer.The reconstruction half of the network is made up ofan additional 6 transposed convolutional layers with batchnormalization applied before each transposed convolution.We use rectiﬁed linear unit (ReLU) as the activation function",
            "refs": [
                32
            ]
        },
        {
            "index": 20,
            "text": "Fig. 3: An example of reconstructed images from the au-toencoder model trained in the house2 environment. Toprow is 3 predicted output images (RGB image appended bydepth image); the bottom row is the original images.",
            "refs": []
        },
        {
            "index": 21,
            "text": "and Adam optimizer  to minimize the mean squarederror between the reconstructed and the original images. Theautoencoder is able to compress a 256 × 256 × 4 RGBDimage into the 4096-d latent space (×64 space savings). Itis then used to encode each image of the state and eachimage of the panoramic goal. A detailed topology of theencoder section is pictured in Figure 4a. An example of theautoencoder performance is shown in Figure 3.",
            "refs": [
                33
            ]
        },
        {
            "index": 22,
            "text": "Our policy model is a fully-connected multilayer percep-tron (MLP) as shown in Figure 4b. We also evaluated theperformance of a variety of other deep learning architecturesincluding convolution along the temporal dimension and longshort-term memory (LSTM) , with different numbers ofpast images in the state and different number of panoramicgoal images, but the MLP architecture with 4 past images and1 current image outperforms the others. Its larger number ofparameters increases its ability to model complex functions.The embeddings of the state and the panoramic goalare ﬁrst concatenated to form a 13 × 4096 matrix andthen progress through 3 fully-connected layers followed bybatch normalization and ReLU activation after each layer,to generate a 16-d vector. The 16-d vector passes throughthe last fully-connected layer to generate 3 logits. Softmaxactivation then outputs a distribution over 3 actions {forward,right, left}. We use Adam optimizer on the cross-entropy lossfor back propagation. At testing, we pick the action withhighest probability deterministically.",
            "refs": [
                34
            ]
        },
        {
            "index": 23,
            "text": "Fig. 4: (a) Encoder architecture of our autoencoder. Pro-gression through each layer consists of a convolution witha stride of 2 followed by batch normalization and ReLUactivation. (b) Policy model architecture. Fuse, FC, andFlatten represent the concatenation operation, a fully-connected layer, and the ﬂatten operation respectively. (c)Goal checking model architecture. Conv1d is the 1D con-volution operation.",
            "refs": []
        },
        {
            "index": 24,
            "text": "This model is created in response to an optimization onour original architecture which had the policy model output adone action when the robotic agent arrives at a goal position.We ﬁnd that the training data is too sparse for the agentto effectively learn identifying a goal location because weonly have one positive example of done at the end of eachtrajectory. Allthe other steps are negative examples fornot done. There is a signiﬁcant imbalance in the numberof positive and negative examples. In addition, at runtimeour robot is likely to arrive at the target position from adifferent viewpoint than those in the panoramic goal images,but during training the policy model receives a view that isone of the panoramic goal images. Thus, we implement anadditional binary classiﬁer to identify whether the agent hasarrived at the goal location. When this model predicts a doneaction the navigation pipeline terminates.",
            "refs": []
        },
        {
            "index": 25,
            "text": "We evaluate our navigation pipeline in 2 environmentsselected from the Stanford2D3DS dataset and 3 environmentsselected from the Matterport3D dataset. Metadata (includingnumber of rooms and area) are shown in Table I. We use theGibson simulator with a Fetch  robot and focus on howthe navigation pipeline generalizes to unseen targets underthe same trained environment. All experiments are conductedusing an NVIDIA 1080Ti GPU. Examples of planned paths,recovery behavior, and experimental environments can befound in the attached video.",
            "refs": []
        },
        {
            "index": 26,
            "text": "For each submodule (autoencoder, policy, and goalchecker) in our navigation pipeline, we need to to generatethe corresponding dataset for training and testing using thecolored meshes loaded in the Gibson simulator. We pickthe model with least loss (autoencoder) or highest accuracy(policy, goal checker) on the test set. Unless otherwisespeciﬁed, we use a learning rate of 0.001.",
            "refs": []
        },
        {
            "index": 27,
            "text": "1) 2D Environment Map: We create a high resolution 2Dmap of the environment from its colored mesh, which enablestrajectory planning using Dijkstra’s algorithm. Each of themaps uses a resolution of 1cm× 1cm per pixel which allowsfor diverse images from many candidate locations. We checkcollision for each candidate location using a bounding boxwith dimensions matching the Fetch robot.",
            "refs": []
        },
        {
            "index": 28,
            "text": "2) Autoencoder: For each environments’ autoencoderdataset, we randomly sample 120K collision-free locationsfrom the generated map and capture RGBD images at thoselocations in the simulator. We use a 0.9 / 0.1 train / test splitand the autoencoder model is trained for 200 epochs whichon average takes 12 GPU hours.",
            "refs": []
        },
        {
            "index": 29,
            "text": "3) Policy: Using Dijktra’s algorithm with costmap op-timization, we are able to generate collision-free experttrajectories. In order to map the trajectory into to a sequenceof { f orward,right,le f t} commands, we step through eachwaypoint position and use a simple discretization strategy.We look ahead 25 steps to determine whether to turn therobot left or right. The robot turns left or right whether thelook-ahead position is turned at an offset greater than 20◦and outputs turns in increments of 10◦. Using only the nextposition would result in the robot constantly recalculating itsdirection and the robot would turn at every step to face a newdirection. Forward commands are given if the robot is fartherfrom the next position than 0.1m, and if so moves the robotforward in increments of 0.1m until under the threshold.",
            "refs": []
        },
        {
            "index": 30,
            "text": "Fig. 5: Eight randomly selected non-overlapping successful trajectories in area1. Blue dots are start positions and greendots are goal positions. Trajectory 5 and 8 show recovery behavior which ultimately leads to a successful trajectory.",
            "refs": []
        },
        {
            "index": 31,
            "text": "each step in the expert trajectories. The average numberof steps per trajectory varies from 41 (house2) to 332(area1). Each training / testing example is constructed bytaking the embeddings from past 4 steps concatenated withthe embedding at the current step. Because larger environ-ments tend to have longer trajectories, we generate 3000trajectories for area1 and area2, 5000 trajectories forhouse17, and 7000 trajectories for house1 and house2to keep the total number of training examples roughly thesame. We use 80% of the trajectories for training and therest for testing. The policy network is trained for 200 epochswhich takes around 90 GPU hours. The average accuracy forthe policy model is 0.91.",
            "refs": []
        },
        {
            "index": 32,
            "text": "We compare our navigation pipeline with an RGBDSLAM  approach and the target-driven deep RL methodfrom . The methods we examine are described below.",
            "refs": [
                9
            ]
        },
        {
            "index": 33,
            "text": "a) SLAM is the Real-Time Appearance-Based Mapping(RTABMap) library provided by , which is anRGBD Graph-Based SLAM approach based on an in-cremental appearance-based loop closure detector. Therobot is not given the map beforehand in our implemen-tation and builds it up as it moves along.",
            "refs": []
        },
        {
            "index": 34,
            "text": "b) Siamese Actor-Critic (SAC) is the method proposedby . We only keep one scene speciﬁc layer and trainthe whole network for each environment as we focuson the generalization to different targets under the sameenvironment. We provide a goal-reaching reward of 10.0upon task completion and a small penalty of −0.01at each time step. We train it on 100 targets with amaximum step size of 10000 for each episode. We giveeach environment a budget of 20M frames (steps).",
            "refs": [
                9
            ]
        },
        {
            "index": 35,
            "text": "c) Ours (GPS) is a variant of our proposed pipeline.Instead of using the learned goal checking model, it usesthe GPS information and the provided goal coordinateto check whether the agent is at the goal.",
            "refs": []
        },
        {
            "index": 36,
            "text": "a), b) and c) assume the agent has an idealized GPS andis provided with the static goal coordinate as in . As aresult, the agent is able to compute the relative position ofthe target at each time step and can use this information tocheck if the goal has been reached.C. Evaluation Criteria",
            "refs": []
        },
        {
            "index": 37,
            "text": "collision at runtime , we simulate real physics andconsider the trial a failure when collision occurs.",
            "refs": [
                9
            ]
        },
        {
            "index": 38,
            "text": "Similar to many previous works on navigation bench-marks , we focus on three evaluationmetrics:",
            "refs": [
                9,
                18
            ]
        },
        {
            "index": 39,
            "text": "2) Success Weighted by Path Length (SPL)  metricis shown in Formula 2, where li is the length of theshortest path between start and goal position, pi is thelength of the observed path taken by an agent, and Siis a binary indicator of success in trial i. This metricweighs each success by the quality of path and thus isalways ≤ Success Rate.1N",
            "refs": [
                18
            ]
        },
        {
            "index": 40,
            "text": "Our proposed navigation pipeline signiﬁcantly outper-forms RGBD SLAM and the state-of-the-art deep RL methodin terms of path quality and success rate, as shown in Table I.See Figure 5 for several example trajectories generated byour method in the area1 environment.",
            "refs": []
        },
        {
            "index": 41,
            "text": "SLAM  struggles to localize itself using RGBD alone,due to the high complexity of our testing environments. Itsucceeds only when the start position is close to the goalposition. SAC  performs much worse than ours due tothe sparse reward and limited number of training frames.In , each environment is a single room and they usesynthetic images but our environment can be up to 1031m2with 40 rooms and we are handling real-world images. Ourenvironment has higher complexity with more obstacles andthe entrances to the rooms can be extremely narrow resultingin a difﬁcult solution. SAC needs millions of frames toconverge in our environment which is not practical. claims they can generalize to new targets by evaluating onlyon 10 targets that are several steps away from the trainingtargets. In our experimental setup the targets can be anywhereon the map. A majority of the successes for SAC is whenthe target location happens to be in a same room as the startlocation. Our method also requires much fewer simulationsteps / training frames (∼ 700K compared to 20M) and lesstraining time (90 GPU hours compared to 300 GPU hours).Our method with no GPS achieves similar performanceto our variant with GPS. As an ablation study, instead ofhaving a separate goal checking model, a done action isgenerated directly from the policy model, and we ﬁnd thatusing a separate goal checking model increases the successrate by 0.2 ∼ 0.5. In the cases where the policy modelincorrectly identiﬁes done, it either outputs done prematurelyor passes the goal withoutterminating. We intentionallykeep the amount of training data roughly the same acrossall environments to evaluate how the performance changes",
            "refs": [
                9
            ]
        },
        {
            "index": 42,
            "text": "TABLE I: Different method results over 5 environments, withthe best values in bold. For SPL higher values are better.For OOR lower values are better. Our method with no GPSachieves similar performance to our variant with GPS. Ourmethods perform best across all environments and metrics.",
            "refs": []
        },
        {
            "index": 43,
            "text": "with the complexity of the environment. When the numberof rooms is over 30, our method starts to struggle to getto the goal. Despite the reduction in performance due toenvironmental complexity, our method performs on average0.556 and 0.442 better in success rate than SLAM and SACrespectively. Given that we achieve high accuracy on smallerenvironments, we believe the performance in area1 andarea2 will go up if trained on more expert trajectories. Afuture direction is to analyze the amount of training dataneeded for a given environmental complexity.",
            "refs": []
        },
        {
            "index": 44,
            "text": "We proposed a navigation pipeline which does not relyon odometry, map, compass or GPS at runtime and ispurely based on the visualinput and a novel 8-imagepanoramic goal. Our method learns from expert trajectoriesgenerated using RGBD maps of several real environments.Using robotic simulators with real data and photo-realisticrendering, we are able to efﬁciently collect a large amountof expert trajectories and train in simulation with real-worlddata, relieving the need of sim-to-real domain adaptation.Our experiments show that the proposed method 1) achievesbetter performance than deep RL and SLAM baselines, espe-cially in complex environments with difﬁcult and long-rangepath solutions, 2) requires fewer training samples and lesstraining time, and 3) can work across different environmentsgiven an RGBD map. Our future work will explore usingsemantic labels as features for learning navigation, and willtest in more complex environments.",
            "refs": []
        },
        {
            "index": 45,
            "text": " A. Khan, C. Zhang, N. Atanasov, K. Karydis, V. Kumar, andD. D. Lee, “Memory augmented control networks,” arXiv preprintarXiv:1709.05706, 2017.",
            "refs": [
                22
            ]
        },
        {
            "index": 46,
            "text": " A. Tamar, Y. Wu, G. Thomas, S. Levine, and P. Abbeel, “Valueiteration networks,” in Advances in Neural Information ProcessingSystems, 2016, pp. 2154–2162.",
            "refs": []
        },
        {
            "index": 47,
            "text": " A. Mousavian, A. Toshev, M. Fiˇser, J. Koˇseck´a, A. Wahid, andJ. Davidson, “Visual representations for semantic target driven naviga-tion,” in 2019 International Conference on Robotics and Automation(ICRA).",
            "refs": [
                24
            ]
        },
        {
            "index": 48,
            "text": " J. Bruce, N. S¨underhauf, P. Mirowski, R. Hadsell, and M. Milford,“One-shot reinforcement learning for robot navigation with interactivereplay,” arXiv preprint arXiv:1711.10137, 2017.",
            "refs": [
                25
            ]
        },
        {
            "index": 49,
            "text": " C. Richter and N. Roy, “Safe visual navigation via deep learning and",
            "refs": []
        },
        {
            "index": 50,
            "text": " L. Lind, “Deep learning navigation for ugvs on forests paths,” 2018. S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser,“Semantic scene completion from a single depth image,” in Pro-ceedings of the IEEE Conference on Computer Vision and PatternRecognition, 2017, pp. 1746–1754.",
            "refs": []
        },
        {
            "index": 51,
            "text": " E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi,“Ai2-thor: An interactive 3d environment for visual ai,” arXiv preprintarXiv:1712.05474, 2017.",
            "refs": [
                29
            ]
        },
        {
            "index": 52,
            "text": " E. Coumans and Y. Bai., “Pybullet, real-time physics simulation",
            "refs": []
        },
        {
            "index": 53,
            "text": " K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for imagerecognition,” in Proceedings of the IEEE conference on computervision and pattern recognition, 2016, pp. 770–778.",
            "refs": []
        },
        {
            "index": 54,
            "text": " J. Jiang, L. Zheng, F. Luo, and Z. Zhang, “Rednet: Residual encoder-decoder network for indoor rgb-d semantic segmentation,” arXivpreprint arXiv:1806.01054, 2018.",
            "refs": [
                32
            ]
        },
        {
            "index": 55,
            "text": " D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-",
            "refs": [
                33
            ]
        },
        {
            "index": 56,
            "text": " S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural",
            "refs": [
                34
            ]
        },
        {
            "index": 57,
            "text": " M. Wise, M. Ferguson, D. King, E. Diehr, and D. Dymesich, “Fetchand freight : Standard platforms for service robot applications,” 2016. M. Labb´e and F. Michaud, “Rtab-map as an open-source lidar andvisual simultaneous localization and mapping library for large-scaleand long-term online operation,” Journal of Field Robotics, vol. 36,no. 2, pp. 416–446, 2019.",
            "refs": []
        },
        {
            "index": 58,
            "text": " D. Mishkin, A. Dosovitskiy, and V. Koltun, “Benchmarking classicand learned navigation in complex 3d environments,” arXiv preprintarXiv:1901.10915, 2019.",
            "refs": []
        },
        {
            "index": 59,
            "text": " M. G. Dissanayake, P. Newman, S. Clark, H. F. Durrant-Whyte,and M. Csorba, “A solution to the simultaneous localization andmap building (slam) problem,” IEEE Transactions on robotics andautomation, vol. 17, no. 3, pp. 229–241, 2001.",
            "refs": [
                1
            ]
        },
        {
            "index": 60,
            "text": " L. Kavraki, P. Svestka, and M. H. Overmars, Probabilistic roadmapsfor path planning in high-dimensional conﬁguration spaces. Un-known Publisher, 1994, vol. 1994.",
            "refs": [
                2
            ]
        },
        {
            "index": 61,
            "text": " S. M. LaValle and J. J. Kuffner Jr, “Rapidly-exploring random trees:",
            "refs": [
                3
            ]
        },
        {
            "index": 62,
            "text": " J. J. Kuffner Jr and S. M. LaValle, “Rrt-connect: An efﬁcient approach",
            "refs": [
                4
            ]
        },
        {
            "index": 63,
            "text": " J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement learning inrobotics: A survey,” The International Journal of Robotics Research,vol. 32, no. 11, pp. 1238–1274, 2013.",
            "refs": [
                5
            ]
        },
        {
            "index": 64,
            "text": " K. M¨ulling, J. Kober, and J. Peters, “A biomimetic approach to robottable tennis,” Adaptive Behavior, vol. 19, no. 5, pp. 359–376, 2011. V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,et al., “Human-level control through deep reinforcement learning,”Nature, vol. 518, no. 7540, p. 529, 2015.",
            "refs": [
                6
            ]
        },
        {
            "index": 65,
            "text": " M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welin-der, B. McGrew, J. Tobin, O. P. Abbeel, and W. Zaremba, “Hindsightexperience replay,” in Advances in Neural Information ProcessingSystems, 2017, pp. 5048–5058.",
            "refs": [
                8
            ]
        },
        {
            "index": 66,
            "text": " Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, andA. Farhadi, “Target-driven visual navigation in indoor scenes usinglearning,” CoRR, vol. abs/1609.05143, 2016.deep reinforcement[Online]. Available: http://arxiv.org/abs/1609.05143",
            "refs": [
                9
            ]
        },
        {
            "index": 67,
            "text": " A. Francis, A. Faust, H.-T. L. Chiang, J. Hsu, J. C. Kew, M. Fiser,and T.-W. E. Lee, “Long-range indoor navigation with prm-rl,” arXivpreprint arXiv:1902.09458, 2019.",
            "refs": [
                10
            ]
        },
        {
            "index": 68,
            "text": " P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard,A. Banino, M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu,D. Kumaran, and R. Hadsell, “Learning to navigate in complexenvironments,” CoRR, vol. abs/1611.03673, 2016. [Online]. Available:http://arxiv.org/abs/1611.03673",
            "refs": [
                11
            ]
        },
        {
            "index": 69,
            "text": " A. Faust, K. Oslund, O. Ramirez, A. Francis, L. Tapia, M. Fiser,and J. Davidson, “Prm-rl: Long-range robotic navigation tasks bycombining reinforcement learning and sampling-based planning,” in2018 IEEE International Conference on Robotics and Automation(ICRA).",
            "refs": [
                12
            ]
        },
        {
            "index": 70,
            "text": " I. Armeni, A. Sax, A. R. Zamir, and S. Savarese, “Joint 2D-3D-Semantic Data for Indoor Scene Understanding,” ArXiv e-prints, Feb.2017.",
            "refs": [
                13
            ]
        },
        {
            "index": 71,
            "text": " A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva,S. Song, A. Zeng, and Y. Zhang, “Matterport3d: Learning from rgb-ddata in indoor environments,” International Conference on 3D Vision(3DV), 2017.",
            "refs": []
        },
        {
            "index": 72,
            "text": " M. Savva, A. X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun,indoor simulator for navigation in complex",
            "refs": [
                15
            ]
        },
        {
            "index": 73,
            "text": " F. Xia, A. R. Zamir, Z. He, A. Sax, J. Malik, and S. Savarese,“Gibson env: Real-world perception for embodied agents,” CoRR,vol. abs/1808.10654, 2018. [Online]. Available: http://arxiv.org/abs/1808.10654",
            "refs": [
                16
            ]
        },
        {
            "index": 74,
            "text": " M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain,J. Straub, J. Liu, V. Koltun, J. Malik, et al., “Habitat: A platform forembodied ai research,” arXiv preprint arXiv:1904.01201, 2019.",
            "refs": []
        },
        {
            "index": 75,
            "text": " P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta,V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, et al.,“On evaluation of embodied navigation agents,” arXiv preprintarXiv:1807.06757, 2018.",
            "refs": [
                18
            ]
        },
        {
            "index": 76,
            "text": " P. Mirowski, M. Grimes, M. Malinowski, K. M. Hermann, K. Ander-son, D. Teplyashin, K. Simonyan, A. Zisserman, R. Hadsell, et al.,“Learning to navigate in cities without a map,” in Advances in NeuralInformation Processing Systems, 2018, pp. 2419–2430.",
            "refs": [
                19
            ]
        },
        {
            "index": 77,
            "text": " H.-T. L. Chiang, A. Faust, M. Fiser, and A. Francis, “Learningnavigation behaviors end-to-end with autorl,” IEEE Robotics andAutomation Letters, vol. 4, no. 2, pp. 2007–2014, 2019.",
            "refs": [
                20
            ]
        },
        {
            "index": 78,
            "text": " S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik, “Cog-nitive mapping and planning for visual navigation,” in Proceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition,2017, pp. 2616–2625.",
            "refs": [
                21
            ]
        }
    ],
    "6": [
        {
            "index": 0,
            "text": "We present an end-to-end framework for solving the Vehicle Routing Problem(VRP) using reinforcement learning. In this approach, we train a single policymodel that ﬁnds near-optimal solutions for a broad range of problem instances ofsimilar size, only by observing the reward signals and following feasibility rules.We consider a parameterized stochastic policy, and by applying a policy gradientalgorithm to optimize its parameters, the trained model produces the solution asa sequence of consecutive actions in real time, without the need to re-train forevery new problem instance. On capacitated VRP, our approach outperformsclassical heuristics and Google’s OR-Tools on medium-sized instances in solutionquality with comparable computation time (after training). We demonstrate howour approach can handle problems with split delivery and explore the effect of suchdeliveries on the solution quality. Our proposed framework can be applied to othervariants of the VRP such as the stochastic VRP, and has the potential to be appliedmore generally to combinatorial optimization problems.",
            "refs": []
        },
        {
            "index": 1,
            "text": "The Vehicle Routing Problem (VRP) is a combinatorial optimization problem that has been studiedin applied mathematics and computer science for decades. VRP is known to be a computationallydifﬁcult problem for which many exact and heuristic algorithms have been proposed, but providingfast and reliable solutions is still a challenging task. In the simplest form of the VRP, a singlecapacitated vehicle is responsible for delivering items to multiple customer nodes; the vehicle mustreturn to the depot to pick up additional items when it runs out. The objective is to optimize a set ofroutes, all beginning and ending at a given node, called the depot, in order to attain the maximumpossible reward, which is often the negative of the total vehicle distance or average service time. Thisproblem is computationally difﬁcult to solve to optimality, even with only a few hundred customernodes , and is classiﬁed as an NP-hard problem. For an overview of the VRP, see, for example,.The prospect of new algorithm discovery, without any hand-engineered reasoning, makes neuralnetworks and reinforcement learning a compelling choice that has the potential to be an importantmilestone on the path toward solving these problems. In this work, we develop a framework withthe capability of solving a wide variety of combinatorial optimization problems using ReinforcementLearning (RL) and show how it can be applied to solve the VRP. For this purpose, we consider theMarkov Decision Process (MDP) formulation of the problem, in which the optimal solution can beviewed as a sequence of decisions. This allows us to use RL to produce near-optimal solutions byincreasing the probability of decoding “desirable” sequences.A naive approach would be to train an instance-speciﬁc policy by considering every instance separately.In this approach, an RL algorithm needs to take many samples, maybe millions of them, from the",
            "refs": [
                12,
                23,
                24,
                33
            ]
        },
        {
            "index": 2,
            "text": "underlying MDP of the problem to be able to produce a good-quality solution. Obviously, thisapproach is not practical since the RL method should be comparable to existing algorithms not onlyin terms of the solution quality but also in terms of runtime. For example, for all of the problemsstudied in this paper, we wish to have a method that can produce near-optimal solutions in less than asecond. Moreover, the policy learned by this naive approach would not apply to instances other thanthe one that was used in the training; after a small perturbation of the problem setting, e.g., changingthe location or demand of a customer, we would need to rebuild the policy from scratch.Therefore, rather than focusing on training a separate policy for every problem instance, we proposea structure that performs well on any problem sampled from a given distribution. This means that ifwe generate a new VRP instance with the same number of nodes and vehicle capacity, and the samelocation and demand distributions as the ones that we used during training, then the trained policywill work well, and we can solve the problem right away, without retraining for every new instance.As long as we approximate the generating distribution of the problem, the framework can be applied.One can view the trained policy as a black-box heuristic (or a meta-algorithm) which generates ahigh-quality solution in a reasonable amount of time.This study is motivated by the recent work by Bello et al. . We have generalized their frameworkto include a wider range of combinatorial optimization problems such as the VRP. Bello et al. propose the use of a Pointer Network  to decode the solution. One major issue that complicatesthe direct use of their approach for the VRP is that it assumes the system is static over time. Incontrast, in the VRP, the demands change over time in the sense that once a node has been visited itsdemand becomes, effectively, zero. To overcome this, we propose an alternate approach—which issimpler than the Pointer Network approach—that can efﬁciently handle both the static and dynamicelements of the system. Our policy model consists of a recurrent neural network (RNN) decodercoupled with an attention mechanism. At each time step, the embeddings of the static elements arethe input to the RNN decoder, and the output of the RNN and the dynamic element embeddings arefed into an attention mechanism, which forms a distribution over the feasible destinations that can bechosen at the next decision point.The proposed framework is appealing to practitioners since we utilize a self-driven learning procedurethat only requires the reward calculation based on the generated outputs; as long as we can observethe reward and verify the feasibility of a generated sequence, we can learn the desired meta-algorithm.For instance, if one does not know how to solve the VRP but can compute the cost of a givensolution, then one can provide the signal required for solving the problem using our method. Unlikemost classical heuristic methods, it is robust to problem changes, e.g., when a customer changesits demand value or relocates to a different position, it can automatically adapt the solution. Usingclassical heuristics for VRP, the entire distance matrix must be recalculated and the system mustbe re-optimized from scratch, which is often impractical, especially if the problem size is large.In contrast, our proposed framework does not require an explicit distance matrix, and only onefeed-forward pass of the network will update the routes based on the new data.Our numerical experiments indicate that our framework performs signiﬁcantly better than well-knownclassical heuristics designed for the VRP, and that it is robust in the sense that its worst results are stillrelatively close to optimal. Comparing our method with the OR-Tools VRP engine , which is oneof the best open-source VRP solvers, we observe a noticeable improvement; in VRP instances with50 and 100 customers, our method provides shorter tours in roughly 61% of the instances. Anotherinteresting observation that we make in this study is that by allowing multiple vehicles to supply thedemand of a single node, our RL-based framework ﬁnds policies that outperform the solutions thatrequire single deliveries. We obtain this appealing property, known as the split delivery, without anyhand engineering and at no extra cost.",
            "refs": [
                34,
                16
            ]
        },
        {
            "index": 3,
            "text": "Sequence-to-Sequence Models Sequence-to-Sequence models  are useful in tasks forwhich a mapping from one sequence to another is required. They have been extensively studied inthe ﬁeld of neural machine translation over the past several years, and there are numerous variants",
            "refs": [
                34,
                25
            ]
        },
        {
            "index": 4,
            "text": "of these models. The general architecture, which is shared by many of these models, consists oftwo RNN networks called the encoder and decoder. An encoder network reads through the inputsequence and stores the knowledge in a ﬁxed-size vector representation (or a sequence of vectors);then, a decoder converts the encoded information back to an output sequence.In the vanilla Sequence-to-Sequence architecture , the source sequence appears only once in theencoder and the entire output sequence is generated based on one vector (i.e., the last hidden stateof the encoder RNN). Other extensions, for example Bahdanau et al. , illustrate that the sourceinformation can be used more explicitly to increase the amount of information during the decodingsteps. In addition to the encoder and decoder networks, they employ another neural network, namelyan attention mechanism that attends to the entire encoder RNN states. This mechanism allows thedecoder to focus on the important locations of the source sequence and use the relevant informationduring decoding steps for producing “better” output sequences. Recently, the concept of attention hasbeen a popular research idea due to its capability to align different objects, e.g., in computer vision and neural machine translation . In this study, we also employ a specialattention structure for the policy parameterization. See Section 3.3 for a detailed discussion of theattention mechanism.",
            "refs": [
                6,
                39,
                18,
                19,
                25
            ]
        },
        {
            "index": 5,
            "text": "Neural Combinatorial Optimization Over the last several years, multiple methods have beendeveloped to tackle combinatorial optimization problems by using recent advances in artiﬁcialintelligence. The ﬁrst attempt was proposed by Vinyals et al. , who introduce the concept ofa Pointer Network, a model originally inspired by sequence-to-sequence models. Because it isinvariant to the length of the encoder sequence, the Pointer Network enables the model to apply tocombinatorial optimization problems, where the output sequence length is determined by the sourcesequence. They use the Pointer Network architecture in a supervised fashion to ﬁnd near-optimalTraveling Salesman Problem (TSP) tours from ground truth optimal (or heuristic) solutions. Thisdependence on supervision prohibits the Pointer Network from ﬁnding better solutions than the onesprovided during the training.Closest to our approach, Bello et al.  address this issue by developing a neural combinatorialoptimization framework that uses RL to optimize a policy modeled by a Pointer Network. Usingseveral classical combinatorial optimization problems such as TSP and the knapsack problem, theyshow the effectiveness and generality of their architecture.On a related topic, Dai et al.  solve optimization problems over graphs using a graph embeddingstructure  and a deep Q-learning (DQN) algorithm . Even though VRP can be representedby a graph with weighted nodes and edges, their proposed approach does not directly apply since inVRP, a particular node (e.g. the depot) might be visited multiple times.Next, we introduce our model, which is a simpliﬁed version of the Pointer Network.",
            "refs": [
                34,
                10,
                26
            ]
        },
        {
            "index": 6,
            "text": "feasibility constraints. For instance, in the VRP that we consider in this work, the terminatingcondition is that there is no more demand to satisfy. This process will generate a sequence of lengthT , Y = {yt, t = 0, ..., T}, possibly with a different sequence length compared to the input length M.This is due to the fact that, for example, the vehicle may have to go back to the depot several times toreﬁll. We also use the notation Yt to denote the decoded sequence up to time t, i.e., Yt = {y0,··· , yt}.We are interested in ﬁnding a stochastic policy π which generates the sequence Y in a way thatminimizes a loss objective while satisfying the problem constraints. The optimal policy π∗ willgenerate the optimal solution with probability 1. Our goal is to make π as close to π∗ as possible.Similar to Sutskever et al. , we use the probability chain rule to decompose the probability ofgenerating sequence Y , i.e., P (Y |X0), as follows:",
            "refs": []
        },
        {
            "index": 7,
            "text": "(3)where g is an afﬁne function that outputs an input-sized vector, and ht is the state of the RNN decoderthat summarizes the information of previously decoded steps y0,··· , yt. We will describe the detailsof our proposed attention mechanism in Section 3.3.Remark 1: This structure can handle combinatorial optimization problems in both a more classicalstatic setting as well as in dynamically changing ones. In static combinatorial optimization, X0 fullydeﬁnes the problem that we are trying to solve. For example, in the VRP, X0 includes all customerlocations as well as their demands, and the depot location; then, the remaining demands are updatedwith respect to the vehicle destination and its load. With this consideration, often there exists awell-deﬁned Markovian transition function f, as deﬁned in (2), which is sufﬁcient to update thedynamics between decision points. However, our framework can also be applied to problems inwhich the state transition function is unknown and/or is subject to external noise, since the trainingdoes not explicitly make use of the transition function. However, knowing this transition functionhelps in simulating the environment that the training algorithm interacts with. See Appendix C.6for an example of how to handle a stochastic version of the VRP in which random customers withrandom demands appear over time.",
            "refs": []
        },
        {
            "index": 8,
            "text": "Although the framework proposed by Bello et al.  works well on problems such as the knapsackproblem and TSP, it is not efﬁcient to more complicated combinatorial optimization problems inwhich the system representation varies over time, such as VRP. Bello et al.  feed a random sequenceof inputs to the RNN encoder. Figure 1 illustrates with an example why using the RNN in the encoderis restrictive. Suppose that at the ﬁrst decision step, the policy sends the vehicle to customer 1,0 (cid:54)= d1and as a result, its demand is satisﬁed, i.e., d11. Then in the second decision step, we need tore-calculate the whole network with the new d11 information in order to choose the next customer. Thedynamic elements complicate the forward pass of the network since there should be encoder/decoderupdates when an input changes. The situation is even worse during back-propagation to accumulatethe gradients since we need to remember when the dynamic elements changed. In order to resolvethis complication, we require the policy model to be invariant to the input sequence so that changingthe order of any two inputs does not affect the network. In Section 3.2, we present a simple networkthat satisﬁes this property.",
            "refs": []
        },
        {
            "index": 9,
            "text": "Figure 2: Our proposed model. The embeddinglayer maps the inputs to a high-dimensionalvector space. On the right, an RNN decoderstores the information of the decoded sequence.Then, the RNN hidden state and embedded in-put produce a probability distribution over thenext input using the attention mechanism.",
            "refs": []
        },
        {
            "index": 10,
            "text": "relative position must be captured in order for the translation to be accurate. But the question here is,why do we need to have them in the encoder for combinatorial optimization problems when there isno meaningful order in the input set? As an example, in the VRP, the inputs are the set of unorderedcustomer locations with their respective demands, and their order is not meaningful; any randompermutation contains the same information as the original inputs. Therefore, in our model, we simplyleave out the encoder RNN and directly use the embedded inputs instead of the RNN hidden states.By this modiﬁcation, many of the computational complications disappear, without decreasing theefﬁciency. In Appendix A, we provide experiments to verify this claim.As illustrated in Figure 2, our model is composed of two main components. The ﬁrst is a set ofgraph embeddings  that can be used to encode structured data inputs. Among the availabletechniques, we tried a one-layer Graph Convolutional Network  embedding, but it did not showany improvement on the results, so we kept the embedding in this paper simple by utilizing the localinformation at each node, e.g., its coordinates and demand values, without incorporating adjacencyinformation. In fact, this embeddings maps each customer’s information into a D-dimensional vectorspace encoding. We might have multiple embeddings corresponding to different elements of theinput, but they are shared among the inputs. The second component is a decoder that points to aninput at every decoding step. As is common in the literature , we use RNN to model thedecoder network. Notice that we feed the static elements as the inputs to the decoder network. Thedynamic element can also be an input to the decoder, but our experiments on the VRP do not suggestany improvement by doing so. For this reason, the dynamic elements are used only in the attentionlayer, described next.",
            "refs": [
                21
            ]
        },
        {
            "index": 11,
            "text": "An attention mechanism is a differentiable structure for addressing different parts of the input. Figure2 illustrates the attention mechanism employed in our method. At decoder step i, we utilize acontext-based attention mechanism with glimpse, similar to Vinyals et al. , which extracts therelevant information from the inputs using a variable-length alignment vector at. In other words, atspeciﬁes how much every input data point might be relevant in the next decoding step t.t) be the embedded input i, and ht ∈ RD be the memory state of the RNN cell atLet ¯xidecoding step t. The alignment vector at is then computed as",
            "refs": [
                35
            ]
        },
        {
            "index": 12,
            "text": "In (4)–(6), va, vc, Wa and Wc are trainable variables.Remark 2: Model Symmetry: Vinyals et al.  discuss an extension of sequence-to-sequencemodels where they empirically demonstrate that in tasks with no obvious input sequence, such assorting, the order in which the inputs are fed into the network matter. A similar concern ariseswhen using Pointer Networks for combinatorial optimization problems. However, the policy modelproposed in this paper does not suffer from such a complication since the embeddings and theattention mechanism are invariant to the input order.",
            "refs": [
                35
            ]
        },
        {
            "index": 13,
            "text": "To train the network, we use well-known policy gradient approaches. To use these methods, weparameterize the stochastic policy π with parameters θ, where θ is vector of all trainable variablesused in embedding, decoder, and attention mechanism. Policy gradient methods use an estimate ofthe gradient of the expected return with respect to the policy parameters to iteratively improve thepolicy. In principle, the policy gradient algorithm contains two networks: (i) an actor network thatpredicts a probability distribution over the next action at any given decision step, and (ii) a criticnetwork that estimates the reward for any problem instance from a given state. Our training methodsare quite standard, and due to space limitation we leave the details to the Appendix.",
            "refs": []
        },
        {
            "index": 14,
            "text": "Many variants of the VRP have been extensively studied in the operations research literature. See,for example, the reviews by Laporte , Laporte et al. , or the book by Toth and Vigo  fordifferent variants of the problem. In this section, we consider a speciﬁc capacitated version of theproblem in which one vehicle with a limited capacity is responsible for delivering items to manygeographically distributed customers with ﬁnite demands. When the vehicle’s load runs out, it returnsto the depot to reﬁll. We will denote the vehicle’s remaining load at time t as lt. The objective is tominimize the total route length while satisfying all of the customer demands. This problem is oftencalled the capacitated VRP (CVRP) to distinguish it from other variants, but we will refer to it simplyas the VRP.We assume that the node locations and demands are randomly generated from a ﬁxed distribution.Speciﬁcally, the customers and depot locations are randomly generated in the unit square .For simplicity of exposition, we assume that the demand of each node is a discrete number in {1, .., 9},chosen uniformly at random. We note, however, that the demand values can be generated from anydistribution, including continuous ones.We assume that the vehicle is located at the depot at time 0, so the ﬁrst input to the decoder isan embedding of the depot location. At each decoding step, the vehicle chooses from among thecustomer nodes or the depot to visit in the next step. After visiting customer node i, the demands andvehicle load are updated as follows:t − lt),",
            "refs": [
                23,
                24,
                33,
                1
            ]
        },
        {
            "index": 15,
            "text": "(7)which is an explicit deﬁnition of the state transition function (2) for the VRP. Once a sequence of thenodes to be visited is sampled, we compute the total vehicle distance and use its negative value as thereward signal.In this experiment, we have employed two different decoders: (i) greedy, in which at every decodingstep, the node (either customer or depot) with the highest probability is selected as the next destination,and (ii) beam search (BS), which keeps track of the most probable paths and then chooses the onewith the minimum tour length . Our results indicate that by applying the beam search algorithm,the quality of the solutions can be improved with only a slight increase in computation time.For faster training and generating feasible solutions, we have used a masking scheme which setsthe log-probabilities of infeasible solutions to −∞ or forces a solution if a particular condition issatisﬁed. In the VRP, we use the following masking procedures: (i) nodes with zero demand arenot allowed to be visited; (ii) all customer nodes will be masked if the vehicle’s remaining load is",
            "refs": [
                28
            ]
        },
        {
            "index": 16,
            "text": "exactly 0; and (iii) the customers whose demands are greater than the current vehicle load are masked.Notice that under this masking scheme, the vehicle must satisfy all of a customer’s demands whenvisiting it. We note, however, that if the situation being modeled does allow split deliveries, onecan relax (iii). Indeed, the relaxed masking allows split deliveries, so the solution can allocate thedemands of a given customer into multiple routes. This property is, in fact, an appealing behaviorthat is present in many real-world applications, but is often neglected in classical VRP algorithms. Inall the experiments of the next section, we do not allow to split demands. Further investigation andillustrations of this property is included in Appendix C.2–C.4.",
            "refs": []
        },
        {
            "index": 17,
            "text": "In this section, we compare the solutions found using our framework with those obtained from theClarke-Wright savings heuristic (CW), the Sweep heuristic (SW), and Google’s optimization tools(OR-Tools). We run our tests on problems with 10, 20, 50 and 100 customer nodes and correspondingvehicle capacity of 20, 30, 40 and 50; for example, VRP10 consists of 10 customer and the defaultvehicle capacity is 20 unless otherwise speciﬁed. The results are based on 1000 instances, sampledfor each problem size.",
            "refs": []
        },
        {
            "index": 18,
            "text": "Figure 3: Parts 3a and 3b show the optimality gap (in percent) using different algorithms/solvers forVRP10 and VRP20. Parts 3c and 3d give the proportion of the samples for which the algorithms inthe rows outperform those in the columns; for example, RL-BS(5) is superior to RL-greedy in 85.8%of the VRP50 instances.",
            "refs": []
        },
        {
            "index": 19,
            "text": "Figure 3 shows the distribution of total tour lengths generated by our method, using greedy andBS decoders, with the number inside the parentheses indicating the beam-width parameter. Inthe experiments, we label our method with the “RL” preﬁx. In addition, we also implementeda randomized version of both heuristic algorithms to improve the solution quality; for Clarke-Wright, the numbers inside the parentheses are the randomization depth and randomization iterationsparameters; and for Sweep, it is the number of random initial angles for grouping the nodes. Finally,we use Google’s OR-Tools , which is a more competitive baseline. See Appendix B for a detaileddiscussion on the baselines.",
            "refs": [
                16
            ]
        },
        {
            "index": 20,
            "text": "RL-GreedyRL-BS(5)RL-BS(10)CW-GreedyCW-Rnd(5,5)CW-Rnd(10,10)SW-BasicSW-Rnd(5)SW-Rnd(10)OR-Tools0102030405060Optimalitygap(inpercent)RL-GreedyRL-BS(5)RL-BS(10)CW-GreedyCW-Rnd(5,5)CW-Rnd(10,10)SW-BasicSW-Rnd(5)SW-Rnd(10)OR-Tools0102030405060Optimalitygap(inpercent)RL-Greedy12.27.299.497.296.397.997.997.941.5RL-BS(5)85.812.599.799.098.799.199.199.154.6RL-BS(10)91.957.799.899.499.299.399.399.360.2CW-Greedy0.60.30.20.00.068.968.968.91.0CW-Rnd(5,5)2.81.00.692.230.484.584.584.53.5CW-Rnd(10,10)3.71.30.897.568.086.886.886.84.7SW-Basic2.10.90.731.115.513.20.00.01.4SW-Rnd(5)2.10.90.731.115.513.20.00.01.4SW-Rnd(10)2.10.90.731.115.513.20.00.01.4OR-Tools58.545.439.899.096.595.398.698.698.6RL-GreedyRL-BS(5)RL-BS(10)CW-GreedyCW-Rnd(5,5)CW-Rnd(10,10)SW-BasicSW-Rnd(5)SW-Rnd(10)OR-ToolsRL-Greedy25.420.899.999.899.799.599.599.544.4RL-BS(5)74.435.3100.0100.099.9100.0100.0100.056.6RL-BS(10)79.261.6100.0100.0100.099.899.899.862.2CW-Greedy0.10.00.00.00.065.265.265.20.0CW-Rnd(5,5)0.20.00.092.632.782.082.082.00.7CW-Rnd(10,10)0.30.10.097.265.885.485.485.40.8SW-Basic0.50.00.234.818.014.60.00.00.0SW-Rnd(5)0.50.00.234.818.014.60.00.00.0SW-Rnd(10)0.50.00.234.818.014.60.00.00.0OR-Tools55.643.437.8100.099.399.2100.0100.0100.0RL-GreedyRL-BS(5)RL-BS(10)CW-GreedyCW-Rnd(5,5)CW-Rnd(10,10)SW-BasicSW-Rnd(5)SW-Rnd(10)OR-ToolsFor small problems of VRP10 and VRP20, it is possible to ﬁnd the optimal solution, which we do bysolving a mixed integer formulation of the VRP . Figures 3a and 3b measure how far the solutionsare far from optimality. The optimality gap is deﬁned as the distance from the optimal objective valuenormalized by the latter. We observe that using a beam width of 10 is the best-performing method;roughly 95% of the instances are at most 10% away from optimality for VRP10 and 13% for VRP20.Even the outliers are within 20–25% of optimality, suggesting that our RL-BS methods are robust incomparison to the other baseline approaches.Since obtaining the optimal objective values for VRP50 and VRP100 is not computationally af-fordable, in Figures 3d and 3d, we compare the algorithms in terms of their winning rate. Eachtable gives the percentage of the instances in which the algorithms in the rows outperform those inthe columns. In other words, the cell corresponding to (A,B) shows the percentage of the samplesin which algorithm A provides shorter tours than B. We observe that the classical heuristics areoutperformed by the other approaches in almost 100% of the samples. Moreover, RL-greedy iscomparable with OR-Tools, but incorporating beam search into our framework increases the winningrate of our approach to above 60%.Figure 4 shows the solution times normalized by the number of customer nodes. We observe thatthis ratio stays almost the same for RL with different decoders. In contrast, the run time for theClarke-Wright and Sweep heuristics increases faster than linearly with the number of nodes. Thisobservation is one motivation for applying our framework to more general combinatorial problems,since it suggests that our method scales well. Even though the greedy Clark-Wright and basic Sweepheuristics are fast for small instances, they do not provide competitive solutions. Moreover, forlarger problems, our framework is faster than the randomized heuristics. We also include the solutiontimes for OR-Tools in the graph, but we should note that OR-Tools is implemented in C++, whichmakes exact time comparisons impossible since the other baselines are implemented in Python. Itis worthwhile to mention that the runtimes reported for the RL methods are for the case when wedecode a single problem at a time. It is also possible to decode all 1000 test problems in a batchwhich will result in approximately 50× speed up. For example, one-by-one decoding of VRP10 for1000 instances takes around 50 seconds, but by passing all 1000 instances to decoder at once, thetotal decoding time decreases to around 1 second on a K80 GPU.Active search is another method used by  to assist with the RL training on a speciﬁc probleminstance in order to iteratively search for a better solution. We do not believe that active search isa practical tool for our problem. One reason is that it is very time-consuming. A second is that weintend to provide a solver that produces solutions by just scoring a trained policy, while active searchrequires a separate training phase for every instance. To test our conjecture that active search will notbe effective for our problem, we implemented active search for VRP10 with samples of size 640 and1280, and the average route length was 4.78 and 4.77 with 15s and 31s solution times per instance,which are far worse than BS decoders. Note that BS(5) and BS(10) give 4.72 and 4.68, in less than0.1s. For this reason, we exclude active search from our comparisons.",
            "refs": [
                33
            ]
        },
        {
            "index": 21,
            "text": "VRP10VRP20VRP50VRP100VRP Size104103102101time / # of customer nodesRL-GreedyRL-BS(5)RL-BS(10)CW-GreedyCW-Rnd(5,5)CW-Rnd(10,10)SW-BasicSW-Rnd(5)SW-Rnd(10)OR-Tools9092949698100102104106108110Number of Customers15.516.016.517.017.518.018.5Average Tour LengthMethodRL-BS(10)OR-Toolsthe generalization when the problems are very different. More speciﬁcally, we use the models trainedfor VRP50-Cap40 and VRP50-Cap50 in order to generate a solution for VRP100-Cap50. UsingBS(10), the average tour length is 18.00 and 17.80, which is still better than the classical heuristics,but worse than OR-Tools. Overall, these two experiments suggest that when the problems are closein terms of the number of customer and vehicle capacity, it is reasonable to expect a near-optimalsolution, but we will see a degradation when the testing problems are very different from the trainingones.",
            "refs": []
        },
        {
            "index": 22,
            "text": "The proposed framework can be extended easily to problems with multiple depots; one only needs toconstruct the corresponding state transition function and masking procedure. It is also possible toinclude various side constraints: soft constraints can be applied by penalizing the rewards, or hardconstraints such as time windows can be enforced through a masking scheme. However, designingsuch a scheme might be a challenging task, possibly harder than solving the optimization problemitself. Another interesting extension is for VRPs with multiple vehicles. In the simplest case inwhich the vehicles travel independently, one must only design a shared masking scheme to avoid thevehicles pointing to the same customer nodes. Incorporating competition or collaboration among thevehicles is also an interesting line of research that relates to multi-agent RL (MARL) .This framework can also be applied to real-time services including on-demand deliveries and taxis.In Appendix C.6, we design an experiment to illustrate the performance of the algorithm on a VRPwhere both customer locations and their demands are subject to change. Our results indicate superiorperformance than the baselines.",
            "refs": [
                5
            ]
        },
        {
            "index": 23,
            "text": "According to the ﬁndings of this paper, our RL algorithm is competitive with state-of-the-art VRPheuristics, and this represents progress toward solving the VRP with RL for real applications. Thefact that we can solve similar-sized instances without retraining for every new instance makes it easyto deploy our method in practice. For example, a vehicle equipped with a processor can use thetrained model and solve its own VRP, only by doing a sequence of pre-deﬁned arithmetic operations.Moreover, unlike many classical heuristics, our proposed method scales well as the problem sizeincreases, and it has superior performance with competitive solution-time. It does not require adistance matrix calculation, which might be computationally cumbersome, especially in dynamicallychanging VRPs. One important discrepancy which is usually neglected by the classical heuristicsis that one or more of the elements of the VRP are stochastic in the real world. In this paper, wealso illustrate that the proposed RL-based method can be applied to a more complicated stochasticversion of the VRP. In summary, we expect that the proposed architecture has signiﬁcant potentialto be used in real-world problems with further improvements and extensions that incorporate otherrealistic constraints.Noting that the proposed algorithm is not limited to VRP, it will be an important topic of futureresearch to apply it to other combinatorial optimization problems such as bin-packing and job-shop orﬂow-shop scheduling. This method is quite appealing since the only requirement is a veriﬁer to ﬁndfeasible solutions and also a reward signal to demonstrate how well the policy is working. Once thetrained policy is available, it can be used many times, without needing to re-train for new problems aslong as they are generated from the training distribution.",
            "refs": []
        },
        {
            "index": 24,
            "text": "References David L Applegate, Robert E Bixby, Vasek Chvatal, and William J Cook. The traveling salesman",
            "refs": [
                1
            ]
        },
        {
            "index": 25,
            "text": " Claudia Archetti and Maria Grazia Speranza. The split delivery vehicle routing problem: asurvey. In The vehicle routing problem: Latest advances and new challenges, pages 103–122.Springer, 2008.",
            "refs": [
                2
            ]
        },
        {
            "index": 26,
            "text": " Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointlylearning to align and translate. In International Conference on Learning Representations, 2015.",
            "refs": []
        },
        {
            "index": 27,
            "text": " Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combina-",
            "refs": []
        },
        {
            "index": 28,
            "text": " Lucian Bu¸soniu, Robert Babuška, and Bart De Schutter. Multi-agent reinforcement learning: Anoverview. In Innovations in multi-agent systems and applications-1, pages 183–221. Springer,2010.",
            "refs": [
                5
            ]
        },
        {
            "index": 29,
            "text": " Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, and Ram Nevatia. Abc-cnn:An attention based convolutional neural network for visual question answering. arXiv preprintarXiv:1511.05960, 2015.",
            "refs": [
                6
            ]
        },
        {
            "index": 30,
            "text": " Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. Conference on Empirical Methods in NaturalLanguage Processing, 2014.",
            "refs": []
        },
        {
            "index": 31,
            "text": " Nicos Christoﬁdes. Worst-case analysis of a new heuristic for the travelling salesman problem.Technical report, Carnegie-Mellon Univ Pittsburgh Pa Management Sciences Research Group,1976.",
            "refs": [
                8
            ]
        },
        {
            "index": 32,
            "text": " Geoff Clarke and John W Wright. Scheduling of vehicles from a central depot to a number of",
            "refs": []
        },
        {
            "index": 33,
            "text": " Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for",
            "refs": [
                10
            ]
        },
        {
            "index": 34,
            "text": " Hanjun Dai, Elias B Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorialoptimization algorithms over graphs. Advances in Neural Information Processing Systems,2017.",
            "refs": []
        },
        {
            "index": 35,
            "text": " Ricardo Fukasawa, Humberto Longo, Jens Lysgaard, Marcus Poggi de Aragão, Marcelo Reis,Eduardo Uchoa, and Renato F Werneck. Robust branch-and-cut-and-price for the capacitatedvehicle routing problem. Mathematical programming, 106(3):491–511, 2006.",
            "refs": [
                12
            ]
        },
        {
            "index": 36,
            "text": " Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforwardIn Proceedings of the Thirteenth International Conference on Artiﬁcial",
            "refs": []
        },
        {
            "index": 37,
            "text": " Fred Glover and Manuel Laguna. Tabu search*. In Handbook of combinatorial optimization,",
            "refs": [
                14
            ]
        },
        {
            "index": 38,
            "text": " Bruce L Golden, Subramanian Raghavan, and Edward A Wasil. The Vehicle Routing Problem:Latest Advances and New Challenges, volume 43. Springer Science & Business Media, 2008.",
            "refs": []
        },
        {
            "index": 39,
            "text": " Inc. Google. Google’s optimization tools (or-tools), 2018. URL https://github.com/",
            "refs": [
                16
            ]
        },
        {
            "index": 40,
            "text": " Inc. Gurobi Optimization. Gurobi optimizer reference manual, 2016. URL http://www.",
            "refs": [
                17
            ]
        },
        {
            "index": 41,
            "text": " Seunghoon Hong, Junhyuk Oh, Honglak Lee, and Bohyung Han. Learning transferrableknowledge for semantic segmentation with deep convolutional neural network. In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition, pages 3204–3212, 2016.",
            "refs": [
                18
            ]
        },
        {
            "index": 42,
            "text": " Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large",
            "refs": [
                19
            ]
        },
        {
            "index": 43,
            "text": " Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna-",
            "refs": [
                20
            ]
        },
        {
            "index": 44,
            "text": " Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional",
            "refs": [
                21
            ]
        },
        {
            "index": 45,
            "text": " Scott Kirkpatrick, C Daniel Gelatt, and Mario P Vecchi. Optimization by simulated annealing.",
            "refs": []
        },
        {
            "index": 46,
            "text": " Gilbert Laporte. The vehicle routing problem: An overview of exact and approximate algorithms.",
            "refs": [
                23
            ]
        },
        {
            "index": 47,
            "text": " Gilbert Laporte, Michel Gendreau, Jean-Yves Potvin, and Frédéric Semet. Classical and modernheuristics for the vehicle routing problem. International transactions in operational research, 7(4-5):285–300, 2000.",
            "refs": [
                24
            ]
        },
        {
            "index": 48,
            "text": " Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. Conference on Empirical Methods in Natural LanguageProcessing, 2015.",
            "refs": [
                25
            ]
        },
        {
            "index": 49,
            "text": " Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc GBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.",
            "refs": [
                26
            ]
        },
        {
            "index": 50,
            "text": " Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap,Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep rein-forcement learning. In International Conference on Machine Learning, pages 1928–1937,2016.",
            "refs": [
                27
            ]
        },
        {
            "index": 51,
            "text": " Graham Neubig. Neural machine translation and sequence-to-sequence models: A tutorial.",
            "refs": [
                28
            ]
        },
        {
            "index": 52,
            "text": " Ulrike Ritzinger, Jakob Puchinger, and Richard F Hartl. A survey on dynamic and stochasticvehicle routing problems. International Journal of Production Research, 54(1):215–231, 2016.",
            "refs": [
                29
            ]
        },
        {
            "index": 53,
            "text": " Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.Computational capabilities of graph neural networks. IEEE Transactions on Neural Networks,20(1):81–102, 2009.",
            "refs": []
        },
        {
            "index": 54,
            "text": " Lawrence V Snyder and Zuo-Jun Max Shen. Fundamentals of Supply Chain Theory. John",
            "refs": [
                31
            ]
        },
        {
            "index": 55,
            "text": " Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural",
            "refs": []
        },
        {
            "index": 56,
            "text": " Paolo Toth and Daniele Vigo. The Vehicle Routing Problem. SIAM, 2002.",
            "refs": [
                33
            ]
        },
        {
            "index": 57,
            "text": " Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural",
            "refs": [
                34
            ]
        },
        {
            "index": 58,
            "text": " Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for",
            "refs": [
                35
            ]
        },
        {
            "index": 59,
            "text": " Christos Voudouris and Edward Tsang. Guided local search and its application to the traveling",
            "refs": []
        },
        {
            "index": 60,
            "text": " Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement",
            "refs": [
                37
            ]
        },
        {
            "index": 61,
            "text": " Anthony Wren and Alan Holliday. Computer scheduling of vehicles from one or more depots",
            "refs": [
                38
            ]
        },
        {
            "index": 62,
            "text": " Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng, and Zheng Zhang. Theapplication of two-level attention models in deep convolutional neural network for ﬁne-grainedimage classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pages 842–850, 2015.",
            "refs": [
                39
            ]
        },
        {
            "index": 63,
            "text": " Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov,Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation withvisual attention. In International Conference on Machine Learning, pages 2048–2057, 2015.",
            "refs": []
        }
    ]
}